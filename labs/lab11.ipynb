{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ceaa05d-85ba-4394-96b0-5d5a790a1b3b",
   "metadata": {},
   "source": [
    "## Course Assignment Instructions\n",
    "You should have Python (version 3.8 or later) and Jupyter Notebook installed to complete this assignment. You will write code in the empty cell/cells below the problem. While most of this will be a programming assignment, some questions will ask you to \"write a few sentences\" in markdown cells. \n",
    "\n",
    "Submission Instructions:\n",
    "\n",
    "Create a labs directory in your personal class repository (e.g., located in your home directory)\n",
    "Clone the class repository\n",
    "Copy this Jupyter notebook file (.ipynb) into your repo/labs directory\n",
    "Make your edits, commit changes, and push to your repository\n",
    "All submissions must be pushed before the due date to avoid late penalties. \n",
    "\n",
    "Labs are graded out of a 100 pts. Each day late is -10. For a max penalty of -50 after 5 days. From there you may submit the lab anytime before the semester ends for a max score of 50.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ebc630-5037-4af2-ae74-7723d0ce6c80",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "We will now write a gradient boosting algorithm from scratch. We will make it as general as possible for regression and classification and we will make use of sci-kit learn's DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584f6b53-f2f5-4f85-acbe-efc24491c381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.base import clone\n",
    "from plotnine import ggplot, aes, geom_line, geom_point, labs, ggtitle, theme_light\n",
    "\n",
    "\n",
    "#Define the sigmoid function.\n",
    "def sigmoid(x):\n",
    "    \"\"\"Standard sigmoid function to map scores to probabilities.\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class GradientBoosting:\n",
    "    def __init__(self, is_classification=False, base_learner=None, neg_gradient=None, M=None, eta=0.3, verbose=True):\n",
    "        \"\"\"\n",
    "        Initializes the gradient boosting model.\n",
    "        \n",
    "        Parameters:\n",
    "          is_classification (bool): Whether the problem is binary classification.\n",
    "          base_learner (estimator): A base learner instance (default: DecisionTreeRegressor with min_samples_leaf=10%).\n",
    "          neg_gradient (function): Function to compute negative gradient.\n",
    "          M (int): Number of boosting iterations (default: 50 for regression, 100 for classification).\n",
    "          eta (float): Learning rate.\n",
    "          verbose (bool): Whether to print progress messages.\n",
    "        \"\"\"\n",
    "        self.is_classification = is_classification\n",
    "        self.M = M if M is not None else (100 if is_classification else 50)\n",
    "        self.eta = eta\n",
    "        self.verbose = verbose\n",
    "        self.base_learner = base_learner\n",
    "        self.neg_gradient = neg_gradient\n",
    "        self.base_learners = []\n",
    "        self.init_val = None  # Initial prediction value\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the gradient boosting model.\n",
    "        \n",
    "        Parameters:\n",
    "          X (array-like): Feature matrix (n_samples x n_features).\n",
    "          y (array-like): Target vector.\n",
    "        \"\"\"\n",
    "        n = X.shape[0]\n",
    "        \n",
    "        #Set default base learner if none provided.\n",
    "        if self.base_learner is None:\n",
    "            #Use a regression tree with min_samples_leaf as 10% of n.\n",
    "            self.base_learner = DecisionTreeRegressor(min_samples_leaf=max(1, int(0.1 * n)))\n",
    "        \n",
    "        #Set default negative gradient function if not provided.\n",
    "        if self.neg_gradient is None:\n",
    "            if self.is_classification:\n",
    "                #For binary classification, negative gradient for log-odds is y - sigmoid(f)\n",
    "                self.neg_gradient = lambda y, f: y - sigmoid(f)\n",
    "            else:\n",
    "                #For regression, derivative of squared error\n",
    "                self.neg_gradient = lambda y, f: 2 * (y - f)\n",
    "        \n",
    "        #Initial prediction:\n",
    "        if self.is_classification:\n",
    "            #For binary classification, start with a log-odds prediction based on the mean response.\n",
    "            p = np.mean(y)\n",
    "            p = np.clip(p, 1e-5, 1 - 1e-5)  #avoid extreme probabilities\n",
    "            self.init_val = np.log(p / (1 - p))\n",
    "        else:\n",
    "            #For regression, start with the mean of y.\n",
    "            self.init_val = np.mean(y)\n",
    "            \n",
    "        #Initialize the prediction array f.\n",
    "        f = np.full(n, self.init_val, dtype=np.float64)\n",
    "        self.train_predictions = [f.copy()]  # record predictions per iteration\n",
    "        \n",
    "        #Boosting iterations:\n",
    "        for m in range(self.M):\n",
    "            #Compute the negative gradient.\n",
    "            neg_grad = self.neg_gradient(y, f)\n",
    "            \n",
    "            #Fit a new base learner to the negative gradient.\n",
    "            tree = clone(self.base_learner)\n",
    "            tree.fit(X, neg_grad)\n",
    "            \n",
    "            #Predict the adjustments from this base learner.\n",
    "            h = tree.predict(X)\n",
    "            #Update the overall prediction.\n",
    "            f += self.eta * h\n",
    "            self.base_learners.append(tree)\n",
    "            self.train_predictions.append(f.copy())\n",
    "            \n",
    "            #Display training progress.\n",
    "            if self.verbose:\n",
    "                if self.is_classification:\n",
    "                    preds = (sigmoid(f) > 0.5).astype(int)\n",
    "                    error = np.mean(preds != y)\n",
    "                    print(f\"Iteration {m+1}/{self.M}, training error: {error:.4f}\")\n",
    "                else:\n",
    "                    rmse = np.sqrt(np.mean((y - f) ** 2))\n",
    "                    print(f\"Iteration {m+1}/{self.M}, RMSE: {rmse:.4f}\")\n",
    "        self.final_prediction = f\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Returns final predictions.\n",
    "        \n",
    "        For regression, returns the predicted numeric values.\n",
    "        For classification, returns class labels (0 or 1).\n",
    "        \"\"\"\n",
    "        n = X.shape[0]\n",
    "        f = np.full(n, self.init_val, dtype=np.float64)\n",
    "        for tree in self.base_learners:\n",
    "            f += self.eta * tree.predict(X)\n",
    "        if self.is_classification:\n",
    "            return (sigmoid(f) > 0.5).astype(int)\n",
    "        else:\n",
    "            return f\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Returns probability estimates for the positive class (only for classification).\n",
    "        \"\"\"\n",
    "        if not self.is_classification:\n",
    "            raise ValueError(\"predict_proba is only available for classification models\")\n",
    "        n = X.shape[0]\n",
    "        f = np.full(n, self.init_val, dtype=np.float64)\n",
    "        for tree in self.base_learners:\n",
    "            f += self.eta * tree.predict(X)\n",
    "        return sigmoid(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16121b36-d378-42d6-8bef-78f8b37b6e6d",
   "metadata": {},
   "source": [
    "Now we test the code in-sample: Create an example for Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1940652e-9df5-48e0-bdce-52dcf33b523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression Example\n",
    "np.random.seed(1)\n",
    "n_samples = \n",
    "n_features = \n",
    "\n",
    "#Generate a random feature matrix and true coefficients.\n",
    "X_reg = \n",
    "beta = \n",
    "\n",
    "#Create a regression target with additive noise.\n",
    "y_reg = \n",
    "\n",
    "#Initialize and fit the gradient boosting model for regression.\n",
    "model_reg =\n",
    "\n",
    "\n",
    "#Obtain predictions.\n",
    "y_reg_pred = \n",
    "\n",
    "#Create a DataFrame for plotting using plotnine.\n",
    "df_reg = pd.DataFrame({'True_y': , 'Predicted_y': })\n",
    "\n",
    "#Plot using plotnine.\n",
    "plot_reg = (ggplot(df_reg, aes(x='True_y', y='Predicted_y')) +\n",
    "            geom_point() +\n",
    "            labs(x='True y', y='Predicted y') +\n",
    "            ggtitle('Regression: True vs Predicted') +\n",
    "            theme_light())\n",
    "plot_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05317972-1886-40a9-afac-df79b93b4c1a",
   "metadata": {},
   "source": [
    "Now let's do this for Classification ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b4db3a-0782-4d5b-8199-0c3517ddddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification Example\n",
    "np.random.seed(1)\n",
    "X_clf = np.random.randn(n_samples, n_features)\n",
    "\n",
    "#Compute logits and convert to probabilities.\n",
    "logits = \n",
    "probs = \n",
    "\n",
    "#Simulate binary outcomes.\n",
    "y_clf = (np.random.rand(n_samples) < probs).astype(int)\n",
    "\n",
    "#Initialize and fit the gradient boosting model for classification.\n",
    "model_clf = \n",
    "\n",
    "\n",
    "y_clf_pred = model_clf.\n",
    "\n",
    "#Display a simple confusion matrix.\n",
    "confusion = pd.crosstab(pd.Series(y_clf, name=\"Actual\"), pd.Series(y_clf_pred, name=\"Predicted\"))\n",
    "print(\"Confusion Matrix:\\n\", confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbc5902-5c7a-40c3-94b1-4cf0a7c01ccd",
   "metadata": {},
   "source": [
    "Using the diamonds dataset split the data into a training, validation, and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4b9a72-9646-48ea-a861-4af9b40bb198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Load the diamonds dataset.\n",
    "diamonds = sns.load_dataset(\"diamonds\")\n",
    "\n",
    "#Convert categorical variables into dummy/indicator variables.\n",
    "diamonds_dummies = pd.get_dummies(diamonds, drop_first=True)\n",
    "\n",
    "#Assume 'price' is the response and split the data:\n",
    "train_data, temp_data = \n",
    "validation_data, test_data = \n",
    "\n",
    "#Extract features and response.\n",
    "X_train = train_data\n",
    "y_train = train_data\n",
    "\n",
    "X_validation = validation_data\n",
    "y_validation = validation_data\n",
    "\n",
    "X_test = test_data\n",
    "y_test = test_data\n",
    "\n",
    "print(\"Training set size:\", )\n",
    "print(\"Validation set size:\", )\n",
    "print(\"Test set size:\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022c634b-ffce-437d-807d-dad8fd856a2b",
   "metadata": {},
   "source": [
    "Using your new gradient boosting function, optimize the number of base learners, M for the diamonds data using a grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a65316-d87b-4608-9d0d-1090f266d00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "M_grid = \n",
    "rmse_list = []\n",
    "\n",
    "for M_val in M_grid:\n",
    "    print(f\"\\nFitting model with M = {M_val}\")\n",
    "    model =\n",
    "    model.fit()\n",
    "    \n",
    "    #Predict on the validation set.\n",
    "    y_pred_val = \n",
    "    \n",
    "    #Compute RMSE.\n",
    "    rmse = np.sqrt(np.mean(()**2))\n",
    "    rmse_list.append(rmse)\n",
    "    print(f\"M = {M_val}, Validation RMSE = {rmse:.4f}\")\n",
    "\n",
    "#Create a DataFrame with the results.\n",
    "results_df = pd.DataFrame({'M': M_grid, 'RMSE': rmse_list})\n",
    "print(\"\\nGrid Search Results:\")\n",
    "print(results_df)\n",
    "\n",
    "#Identify the best M value (with the lowest RMSE).\n",
    "best_row =\n",
    "best_M = best_row['M']\n",
    "best_RMSE = best_row['RMSE']\n",
    "print(f\"\\nBest M: {best_M} with RMSE: {best_RMSE:.4f}\")\n",
    "\n",
    "plot = (ggplot(results_df, aes(x='M', y='RMSE')) +\n",
    "        geom_line() +\n",
    "        geom_point() +\n",
    "        labs(x=\"Number of Base Learners (M)\", y=\"Validation RMSE\") +\n",
    "        ggtitle(\"Grid Search for Optimal M\") +\n",
    "        theme_light())\n",
    "\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7b888d-77a8-4a2e-937e-f06b6615b749",
   "metadata": {},
   "source": [
    "Now find the error in the test set and comment on its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fb9723-b0e8-4b3e-82f6-592c379c262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the final model using the best M from grid search.\n",
    "print(f\"\\nTraining final model with best_M = {best_M}\")\n",
    "final_model = \n",
    "final_model.fit()\n",
    "\n",
    "#Predict diamond prices on the test set.\n",
    "y_test_pred = final_model.\n",
    "\n",
    "#Compute the test RMSE.\n",
    "test_rmse = np.sqrt(np.mean() ** 2))\n",
    "print(f\"\\nTest RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "#Create a DataFrame for the test set to use with plotnine.\n",
    "df_test = pd.DataFrame({\n",
    "    'Actual Price': ,\n",
    "    'Predicted Price': \n",
    "})\n",
    "\n",
    "#Plot Actual vs Predicted Prices on the test set using plotnine.\n",
    "test_plot = (ggplot(df_test, aes(x='Actual Price', y='Predicted Price')) +\n",
    "             geom_point() +\n",
    "             labs(x='Actual Price', y='Predicted Price') +\n",
    "             ggtitle('Test Set: Actual vs Predicted Prices') +\n",
    "             theme_light())\n",
    "test_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322f19ea-d384-4dca-92d7-29e54b88235e",
   "metadata": {},
   "source": [
    "The test RMSE printed above indicates the average error in predicting diamond prices on unseen data. A lower RMSE suggests that the model's predictions are close to the actual values. If the test RMSE is comparable to the validation RMSE from the grid search, it implies that the model generalizes well to new data.\n",
    "\n",
    "On the other hand, if the test RMSE is significantly higher, it may be a sign that the model is overfitting to the training data, and further tuning or model improvements might be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1274b123-b36b-4522-8c96-21d8a8cea1fc",
   "metadata": {},
   "source": [
    "Now we will assess our custom function performance against the XGBoost package in python. Uncomment the cell below to install xgboost and then run the following cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eed118c-dd56-4c0c-b190-428b66b85907",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842a32c6-90a2-4478-8e71-0c6f4fa3cc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#Split the dataset into training, validation, and test set, we will use 2000 samples for training, 2000 for validation, and the rest as test.\n",
    "train_data, temp_data = train_test_split(diamonds_dummies, train_size=2000, random_state=1)\n",
    "validation_data, test_data = train_test_split(temp_data, train_size=2000, random_state=1)\n",
    "\n",
    "#Separate features and the target (price)\n",
    "X_train = train_data.drop(\"price\", axis=1)\n",
    "y_train = train_data[\"price\"]\n",
    "\n",
    "X_val = validation_data.drop(\"price\", axis=1)\n",
    "y_val = validation_data[\"price\"]\n",
    "\n",
    "X_test = test_data.drop(\"price\", axis=1)\n",
    "y_test = test_data[\"price\"]\n",
    "\n",
    "\n",
    "#Grid search over n_estimators (M) for XGBoost using training and validation sets\n",
    "M_grid = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "rmse_list = []\n",
    "\n",
    "for M_val in M_grid:\n",
    "    print(f\"\\nTraining XGBoost with n_estimators = {M_val}\")\n",
    "    model = xgb.XGBRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    #Predict on the validation set\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    \n",
    "    #Compute RMSE on the validation set\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "    rmse_list.append(rmse)\n",
    "    print(f\"n_estimators = {M_val}, Validation RMSE = {rmse:.4f}\")\n",
    "\n",
    "#Create a DataFrame for the grid search results.\n",
    "results_df = pd.DataFrame({'M': M_grid, 'RMSE': rmse_list})\n",
    "print(\"\\nGrid Search Results:\")\n",
    "print(results_df)\n",
    "\n",
    "#Identify the best M value (the n_estimators with the lowest validation RMSE)\n",
    "best_row = results_df.loc[results_df['RMSE'].idxmin()]\n",
    "best_M = int(best_row['M'])\n",
    "best_RMSE = best_row['RMSE']\n",
    "print(f\"\\nBest n_estimators (M): {best_M} with Validation RMSE: {best_RMSE:.4f}\")\n",
    "\n",
    "#Plot the grid search results using plotnine.\n",
    "plot_grid = (ggplot(results_df, aes(x='M', y='RMSE')) +\n",
    "             geom_line() +\n",
    "             geom_point() +\n",
    "             labs(x=\"Number of Base Learners (n_estimators)\", y=\"Validation RMSE\") +\n",
    "             ggtitle(\"XGBoost Grid Search for Optimal n_estimators\") +\n",
    "             theme_light())\n",
    "plot_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fb6417-682f-45e4-99ba-82c945c7e1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the final XGBoost model using the best M on combined training and validation sets \n",
    "#Combine training and validation sets to make the most out of available data.\n",
    "X_train_val = \n",
    "y_train_val = \n",
    "\n",
    "final_model = xgb.XGBRegressor()\n",
    "final_model.fit(X_train_val, y_train_val)\n",
    "\n",
    "#Evaluate the final model on the test set\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "print(f\"\\nFinal XGBoost Test RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "#Plot Actual vs Predicted Prices on the test set using plotnine.\n",
    "df_test = pd.DataFrame({\n",
    "    'Actual Price': y_test,\n",
    "    'Predicted Price': y_test_pred\n",
    "})\n",
    "\n",
    "plot_test = (ggplot(df_test, aes(x='Actual Price', y='Predicted Price')) +\n",
    "             geom_point() +\n",
    "             labs(x='Actual Price', y='Predicted Price') +\n",
    "             ggtitle('XGBoost: Actual vs Predicted Prices on Test Set') +\n",
    "             theme_light())\n",
    "plot_test\n",
    "\n",
    "# Comments on Performance:\n",
    "# The grid search above selected the best n_estimators (M) for the XGBoost model\n",
    "# based on validation RMSE. The final model, trained on the combined training and\n",
    "# validation data, achieved a Test RMSE as printed above. Comparing this RMSE with\n",
    "# that obtained from the custom gradient boosting function will help assess which\n",
    "# approach generalizes better on unseen diamond price data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc541b5-223a-4ece-b758-07b224e25941",
   "metadata": {},
   "source": [
    "Repeat this exercise for the adult dataset. First create the splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569ec897-8b00-430c-bb73-1070a1a29c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "from plotnine import ggplot, aes, geom_line, geom_point, geom_bar, labs, ggtitle, theme_light, scale_color_manual\n",
    "                      \n",
    "#Adjust the file path to where your adult.csv is located on your desktop.\n",
    "adult_df = pd.read_csv('adult_data.csv')\n",
    "\n",
    "#Replace missing value markers if needed (the adult dataset sometimes has \" ?\" entries)\n",
    "adult_df = adult_df.replace(' ?', np.nan).dropna()\n",
    "\n",
    "#Strip extra whitespace from string columns.\n",
    "adult_df.columns = adult_df.columns.str.strip()\n",
    "for col in adult_df.select_dtypes(include='object').columns:\n",
    "    adult_df[col] = adult_df[col].str.strip()\n",
    "\n",
    "#Convert the target (\"income\") to binary.\n",
    "#Assume income values are in the form \">50K\" and \"<=50K\" (or with period at the end).\n",
    "adult_df['income'] = adult_df['income'].apply(lambda x: 1 if '>50K' in x else 0)\n",
    "\n",
    "#Convert categorical variables to dummy variables.\n",
    "adult_dummies = pd.get_dummies(adult_df, drop_first=True)\n",
    "\n",
    "#Separate features and target.\n",
    "X_adult = adult_dummies.drop(\"income\", axis=1)\n",
    "y_adult = adult_dummies[\"income\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da21d1b2-dade-4657-b5a8-06d31c612bf4",
   "metadata": {},
   "source": [
    "Using your new gradient boosting function, optimize the number of base learners, M for the diamonds data using a grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81bfce8-92d8-4a65-a7d2-822e99f5c18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we use 60% for training, 20% for validation, and 20% for test.\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_adult, y_adult,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=1,\n",
    "                                                    stratify=y_adult)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp,\n",
    "                                                test_size=0.5,\n",
    "                                                random_state=1,\n",
    "                                                stratify=y_temp)\n",
    "\n",
    "#GRID SEARCH FOR BEST NUMBER OF BASE LEARNERS (M) FOR XGBOOST CLASSIFIER\n",
    "M_grid = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "val_acc_list = []\n",
    "\n",
    "print(\"XGBoost Grid Search:\")\n",
    "for M in M_grid:\n",
    "    print(f\"\\nTraining XGBoost classifier with n_estimators = {M}\")\n",
    "    model = xgb.XGBClassifier(objective='binary:logistic',\n",
    "                              n_estimators=M,\n",
    "                              learning_rate=0.3,\n",
    "                              eval_metric='logloss',\n",
    "                              random_state=1)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    acc = accuracy_score(y_val, y_val_pred)\n",
    "    val_acc_list.append(acc)\n",
    "    print(f\"n_estimators = {M}, Validation Accuracy = {acc:.4f}\")\n",
    "\n",
    "results_df = pd.DataFrame({'M': M_grid, 'Validation_Accuracy': val_acc_list})\n",
    "print(\"\\nXGBoost Grid Search Results:\")\n",
    "print(results_df)\n",
    "best_row = results_df.loc[results_df['Validation_Accuracy'].idxmax()]\n",
    "best_M_xgb = int(best_row['M'])\n",
    "best_acc_xgb = best_row['Validation_Accuracy']\n",
    "print(f\"\\nBest n_estimators for XGBoost: {best_M_xgb} with Accuracy: {best_acc_xgb:.4f}\")\n",
    "\n",
    "#Plot the grid search results.\n",
    "plot_grid = (ggplot(results_df, aes(x='M', y='Validation_Accuracy')) +\n",
    "             geom_line() +\n",
    "             geom_point() +\n",
    "             labs(x=\"Number of Base Learners (n_estimators)\", y=\"Validation Accuracy\") +\n",
    "             ggtitle(\"XGBoost Grid Search for Optimal n_estimators\") +\n",
    "             theme_light())\n",
    "plot_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62210bc3-50fa-40a5-9cef-6079519e5681",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN FINAL MODELS ON COMBINED TRAINING + VALIDATION DATA\n",
    "X_train_val = pd.concat([X_train, X_val])\n",
    "y_train_val = pd.concat([y_train, y_val])\n",
    "\n",
    "#Final XGBoost Model\n",
    "final_xgb = xgb.XGBClassifier(objective='binary:logistic',\n",
    "                              n_estimators=best_M_xgb,\n",
    "                              learning_rate=0.3,\n",
    "                              eval_metric='logloss',\n",
    "                              random_state=1)\n",
    "final_xgb.fit(X_train_val, y_train_val)\n",
    "y_test_pred_xgb = final_xgb.predict(X_test)\n",
    "acc_xgb = accuracy_score(y_test, y_test_pred_xgb)\n",
    "y_test_proba_xgb = final_xgb.predict_proba(X_test)[:,1]\n",
    "fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_test_proba_xgb)\n",
    "auc_xgb = auc(fpr_xgb, tpr_xgb)\n",
    "print(f\"\\nFinal XGBoost Test Accuracy: {acc_xgb:.4f}\")\n",
    "print(f\"XGBoost Test AUC: {auc_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e455b3-7ff2-4e07-842d-eabb8333bb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final Custom Gradient Boosting Model ---\n",
    "custom_val_acc_list = []\n",
    "\n",
    "print(\"\\nCustom Model Grid Search:\")\n",
    "for M in M_grid:\n",
    "    print(f\"\\nTraining Custom Gradient Boosting classifier with M = {M}\")\n",
    "    custom_model = GradientBoosting(is_classification=True, M=M, verbose=False)\n",
    "    \n",
    "    #Use numpy arrays as input.\n",
    "    custom_model.fit(X_train.values, y_train.values)\n",
    "    y_val_pred_custom = custom_model.predict(X_val.values)\n",
    "    acc_custom = np.mean(y_val.values == y_val_pred_custom)\n",
    "    custom_val_acc_list.append(acc_custom)\n",
    "    print(f\"M = {M}, Validation Accuracy = {acc_custom:.4f}\")\n",
    "\n",
    "results_custom_df = pd.DataFrame({'M': M_grid, 'Validation_Accuracy': custom_val_acc_list})\n",
    "print(\"\\nCustom Model Grid Search Results:\")\n",
    "print(results_custom_df)\n",
    "best_row_custom = results_custom_df.loc[results_custom_df['Validation_Accuracy'].idxmax()]\n",
    "best_M_custom = int(best_row_custom['M'])\n",
    "best_acc_custom = best_row_custom['Validation_Accuracy']\n",
    "print(f\"\\nBest M for Custom Model: {best_M_custom} with Accuracy: {best_acc_custom:.4f}\")\n",
    "\n",
    "plot_custom_grid = (ggplot(results_custom_df, aes(x='M', y='Validation_Accuracy')) +\n",
    "                    geom_line() +\n",
    "                    geom_point() +\n",
    "                    labs(x=\"Number of Base Learners (M)\", y=\"Validation Accuracy\") +\n",
    "                    ggtitle(\"Custom Model Grid Search for Optimal M\") +\n",
    "                    theme_light())\n",
    "plot_custom_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9da884-498b-41d8-8a18-e2cba97d8bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the final custom model on combined training+validation data.\n",
    "final_custom = GradientBoosting(is_classification=True, M=best_M_custom, verbose=True)\n",
    "final_custom.fit(X_train_val.values, y_train_val.values)\n",
    "y_test_pred_custom = final_custom.predict(X_test.values)\n",
    "acc_custom_final = np.mean(y_test.values == y_test_pred_custom)\n",
    "y_test_proba_custom = final_custom.predict_proba(X_test.values)\n",
    "fpr_custom, tpr_custom, _ = roc_curve(y_test, y_test_proba_custom)\n",
    "auc_custom = auc(fpr_custom, tpr_custom)\n",
    "print(f\"\\nFinal Custom Model Test Accuracy: {acc_custom_final:.4f}\")\n",
    "print(f\"Custom Model Test AUC: {auc_custom:.4f}\")\n",
    "\n",
    "#PERFORMANCE COMPARISON: PLOTTING ROC CURVES & BARCHARTS FOR ACCURACY AND AUC\n",
    "\n",
    "#Prepare ROC data for XGBoost.\n",
    "df_roc_xgb = pd.DataFrame({\n",
    "    'FPR': fpr_xgb,\n",
    "    'TPR': tpr_xgb,\n",
    "    'Model': 'XGBoost'\n",
    "})\n",
    "#Prepare ROC data for the Custom model.\n",
    "df_roc_custom = pd.DataFrame({\n",
    "    'FPR': fpr_custom,\n",
    "    'TPR': tpr_custom,\n",
    "    'Model': 'Custom'\n",
    "})\n",
    "df_roc = pd.concat([df_roc_xgb, df_roc_custom])\n",
    "\n",
    "roc_plot = (ggplot(df_roc, aes(x='FPR', y='TPR', color='Model')) +\n",
    "            geom_line(size=1) +\n",
    "            labs(x='False Positive Rate', y='True Positive Rate',\n",
    "                 title='ROC Curves: XGBoost vs. Custom Gradient Boosting') +\n",
    "            theme_light() +\n",
    "            scale_color_manual(values={'XGBoost': 'blue', 'Custom': 'red'}))\n",
    "roc_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cead063c-696e-45f4-a5ec-433a1998a82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare overall test performance with a bar chart.\n",
    "results_compare = pd.DataFrame({\n",
    "    'Model': ['XGBoost', 'Custom'],\n",
    "    'Test Accuracy': [acc_xgb, acc_custom_final],\n",
    "    'Test AUC': [auc_xgb, auc_custom]\n",
    "})\n",
    "print(\"\\nTest Performance Comparison:\")\n",
    "print(results_compare)\n",
    "\n",
    "accuracy_plot = (ggplot(results_compare, aes(x='Model', y='Test Accuracy', fill='Model')) +\n",
    "                 geom_bar(stat='identity') +\n",
    "                 labs(title='Test Accuracy Comparison') +\n",
    "                 theme_light())\n",
    "accuracy_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604673a4-bf98-4360-9efb-eec01e68b4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_plot = (ggplot(results_compare, aes(x='Model', y='Test AUC', fill='Model')) +\n",
    "            geom_bar(stat='identity') +\n",
    "            labs(title='Test AUC Comparison') +\n",
    "            theme_light())\n",
    "auc_plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
