{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b719b667-569f-4bc1-a845-2960b07b7ff0",
   "metadata": {},
   "source": [
    "## Course Assignment Instructions\n",
    "You should have Python (version 3.8 or later) and Jupyter Notebook installed to complete this assignment. You will write code in the empty cell/cells below the problem. While most of this will be a programming assignment, some questions will ask you to \"write a few sentences\" in markdown cells. \n",
    "\n",
    "Submission Instructions:\n",
    "\n",
    "Create a labs directory in your personal class repository (e.g., located in your home directory)\n",
    "Clone the class repository\n",
    "Copy this Jupyter notebook file (.ipynb) into your repo/labs directory\n",
    "Make your edits, commit changes, and push to your repository\n",
    "All submissions must be pushed before the due date to avoid late penalties. \n",
    "\n",
    "Labs are graded out of a 100 pts. Each day late is -10. For a max penalty of -50 after 5 days. From there you may submit the lab anytime before the semester ends for a max score of 50.  \n",
    "\n",
    "Lab 7 is due on 4/21/25 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fab03d8-34c4-493a-b75f-3c8e831b90d2",
   "metadata": {},
   "source": [
    "## Polynomial Regression and Interaction Regression\n",
    "\n",
    "We will work with the diamonds dataset again. Here we load up the dataset and convert all factors to nominal type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ced0f47-5d01-4cb0-a046-bf97394782aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from plotnine.data import diamonds\n",
    "\n",
    "# Load the diamonds dataset and create a copy\n",
    "diamonds = diamonds.copy()\n",
    "\n",
    "# Convert factors to nominal (unordered categorical) type\n",
    "diamonds['cut'] = pd.Categorical(diamonds['cut'], ordered=False)\n",
    "diamonds['color'] =\n",
    "diamonds['clarity'] = \n",
    "\n",
    "# Print summary statistics similar to skimr\n",
    "print(\"Summary Statistics:\")\n",
    "\n",
    "\n",
    "# Calculate and print the number of missing values per column\n",
    "missing_counts = \n",
    "print(\"\\nMissing Values:\")\n",
    "print(missing_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03ca454-e3d0-4b36-8e71-f56fbe4d03d7",
   "metadata": {},
   "source": [
    "Given the information above, what are the number of columns in the raw X matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36acace2-1d68-4c36-bd7d-304ad0c3122c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc9f36c6-6175-4562-bd05-69c189aa6225",
   "metadata": {},
   "source": [
    "Verify this using code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e017a749-384e-4d13-9322-853c8ef10ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3074e9-5228-494a-8510-05673488587a",
   "metadata": {},
   "source": [
    "Would it make sense to use polynomial expansions for the variables cut, color and clarity? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831b9625-e919-4f4b-9b67-50fc1726aae9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cd77c4c-b325-49c6-8b3d-5e9b03c9d8ec",
   "metadata": {},
   "source": [
    "Would it make sense to use log transformations for the variables cut, color and clarity? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3286ea8-afe7-40e5-b909-107732922577",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49dfb761-d6c2-494c-9217-c1b4b5ec8e78",
   "metadata": {},
   "source": [
    "In order to ensure there is no time trend in the data, randomize the order of the diamond observations in D:."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e79446-74a7-4c67-910f-e8d335b11ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomize the order of the diamond observations\n",
    "diamonds_shuffled = \n",
    "\n",
    "# View the first few rows of the shuffled dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ea5dc3-6354-469a-9b89-34971e7a476a",
   "metadata": {},
   "source": [
    "Let's also concentrate only on diamonds with <= 2 carats to avoid the issue we saw with the maximum. So subset the dataset. Create a variable n equal to the number of remaining rows as this will be useful for later. Then plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379db5f0-aed9-4892-ad4d-51a57bf47a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import ggplot, aes, geom_point, labs, theme_minimal\n",
    "\n",
    "# Subset to diamonds with <= 2 carats\n",
    "\n",
    "\n",
    "# Create a variable n equal to the number of rows in the subset\n",
    "print(diamonds_shuffled.shape[0]) # number of rows before exluding carat size > 2\n",
    "n = diamonds_subset.shape[0]\n",
    "print(\"Number of remaining rows:\", n)\n",
    "\n",
    "# Plot: For example, plot carat vs price\n",
    "p = (ggplot(diamonds_subset, aes(x='carat', y='price')) +\n",
    "\n",
    "     \n",
    "     theme_minimal())\n",
    "\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532f3151-e508-4b81-93e3-4b848770f965",
   "metadata": {},
   "source": [
    "Create a linear model of price ~ carat and gauge its in-sample performance using s_e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf91e43-260e-4f3a-827b-69833ad3e25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Define the independent variable (carat) and the dependent variable (price)\n",
    "X = diamonds_subset['carat']\n",
    "y = diamonds_subset['price']\n",
    "\n",
    "# Add a constant term to the independent variable for the intercept\n",
    "\n",
    "\n",
    "# Fit the linear model using OLS (Ordinary Least Squares)\n",
    "\n",
    "\n",
    "# Extract the residual standard error (s_e) \n",
    "# In statsmodels, the scale attribute gives the variance of the residuals, so we take the square root.\n",
    "\n",
    "\n",
    "# Print the residual standard error\n",
    "print(\"Residual standard error (s_e):\", s_e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fe16e0-8f90-4c44-9bb0-33c7cb161203",
   "metadata": {},
   "source": [
    "Create a model of price ~ clarity and gauge its in-sample performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e453162-050e-46c9-941d-634f828a490f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an explicit copy of diamonds_subset if not already done\n",
    "\n",
    "\n",
    "# Convert the 'clarity' categorical variable into numeric codes\n",
    "# This creates a new column 'clarity_numeric' where each level is mapped to an integer.\n",
    "\n",
    "\n",
    "# Prepare the independent variable (include a constant)\n",
    "\n",
    "\n",
    "# Dependent variable\n",
    "\n",
    "\n",
    "# Fit the linear model using OLS\n",
    "model2 = sm.OLS(y, X).fit()\n",
    "\n",
    "# Extract the residual standard error (s_e)\n",
    "s_e = np.sqrt(model2.scale)\n",
    "\n",
    "# Print the residual standard error\n",
    "print(\"Residual standard error (s_e):\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf064c5-ce0c-4c47-a5f8-ca0492d90f10",
   "metadata": {},
   "source": [
    "Why is the model price ~ carat substantially more accurate than price ~ clarity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40249c4e-d378-45eb-a7bc-9a171c35a13e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94a7ac5f-64ad-499d-a6aa-82fc83b7d441",
   "metadata": {},
   "source": [
    "Create a new transformed feature ln_carat and plot it vs price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074ca1ff-c582-41cf-b673-dd75f23bdb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an explicit copy of diamonds_subset if not already done\n",
    "diamonds_ln_subset = diamonds_subset.copy()\n",
    "\n",
    "# Create a new column with the natural logarithm of carat\n",
    "\n",
    "\n",
    "# Plot ln_carat vs price using plotnine\n",
    "plot = \n",
    "\n",
    "\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0052a2d9-5ce9-4b41-88f2-14f50ee8d7e8",
   "metadata": {},
   "source": [
    "Would price ~ ln_carat be a better fitting model than price ~ carat? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84644412-7fbf-4e68-8536-48f86f93bc6a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a514ad5-1228-411c-9962-44e8f8b06bfe",
   "metadata": {},
   "source": [
    "Verify this by comparing R^2 and RMSE of the two models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b25cc0-2f5d-46d2-aaa5-e4c55a06a9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 1 price ~ carat\n",
    "X = diamonds_subset['carat']\n",
    "y = diamonds_subset['price']\n",
    "\n",
    "# Add a constant term to the independent variable for the intercept\n",
    "r2_model1 = \n",
    "rmse_model1 = \n",
    "\n",
    "#Model 2 price ~ ln_carat\n",
    "r2_model2 = \n",
    "rmse_model2 = \n",
    "\n",
    "# Print R² and RMSE for both models\n",
    "print(\"Model price ~ carat:\")\n",
    "print(\"  R² =\", r2_model1)\n",
    "print(\"  RMSE =\", rmse_model1, \"\\n\")\n",
    "\n",
    "print(\"Model price ~ ln(carat):\")\n",
    "print(\"  R² =\", r2_model2)\n",
    "print(\"  RMSE =\", rmse_model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc11d0ae-8105-4141-bd01-47b1426991eb",
   "metadata": {},
   "source": [
    "Create a new transformed feature ln_price and plot its estimated density:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e760ea66-028b-47e1-ae79-813c12d7677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import geom_histogram\n",
    "\n",
    "# Create the ln_price feature by taking the natural logarithm of price\n",
    "\n",
    "\n",
    "# Plot the histogram of ln_price with a binwidth of 0.01 using plotnine\n",
    "p = \n",
    "\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc32a1b5-db79-4109-a75d-9728123342f5",
   "metadata": {},
   "source": [
    "Now plot it vs carat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfe28ea-ed65-4adb-a79e-c2b1a99f6ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ln_price vs carat using plotnine\n",
    "plot = \n",
    "\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb2ec75-5b2a-4724-850f-694e4d3d6c9f",
   "metadata": {},
   "source": [
    "Would ln_price ~ carat be a better fitting model than price ~ carat? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa29aef-b217-4080-b4f5-df6c7a04248e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f45b30d4-4fcf-41b4-b54f-848fcce25c60",
   "metadata": {},
   "source": [
    "Verify this by computing s_e of this new model. Make sure these metrics can be compared apples-to-apples with the previous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ef8ba3-42c4-4b7c-a809-5c5cea396f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the linear model on the log scale\n",
    "\n",
    "\n",
    "\n",
    "# Metrics on the log scale\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Back-transform predictions to original price scale\n",
    "pred_ln_price = \n",
    "pred_price = \n",
    "\n",
    "# Compute RMSE on the original price scale\n",
    "resid_price = \n",
    "rmse_orig = \n",
    "\n",
    "# Compute R² on the original price scale\n",
    "price = \n",
    "tss =    # Total Sum of Squares\n",
    "rss =         # Residual Sum of Squares\n",
    "r2_orig = \n",
    "\n",
    "# Print the results\n",
    "print(\"Model ln(price) ~ carat (Log Scale):\")\n",
    "print(\"  R² (log scale)  =\", r2_log)\n",
    "print(\"  RMSE (log scale) =\", rmse_log)\n",
    "print(\"\\nModel ln(price) ~ carat (Original Price Scale):\")\n",
    "print(\"  R² (original scale) =\", r2_orig)\n",
    "print(\"  RMSE (original scale) =\", rmse_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c479dd0d-3e0f-4769-b267-2c6f1532f78e",
   "metadata": {},
   "source": [
    "We just compared in-sample statistics to draw a conclusion on which model has better performance. But in-sample statistics can lie! Why is what we did valid?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3724fda-b972-4e4c-9ec0-1f5c84e5ef5a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1002375e-8098-41fb-b677-414448f989db",
   "metadata": {},
   "source": [
    "Plot ln_price vs ln_carat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12154b72-f3d1-4523-b9e8-11ee4db3ec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformed features\n",
    "\n",
    "\n",
    "# Plot ln_price vs ln_carat using plotnine\n",
    "\n",
    "\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08af4065-9221-47cb-b6e9-10c5bdc41b52",
   "metadata": {},
   "source": [
    "Would ln_price ~ ln_carat be the best fitting model than the previous three we considered? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8160e1f8-dff8-4b76-b946-c9c8a37f273b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ff9c112-8651-4166-9e89-de3943b284f1",
   "metadata": {},
   "source": [
    "Verify this by computing s_e of this new model. Make sure these metrics can be compared apples-to-apples with the previous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22962d9b-60d4-4e0c-878b-2aec2e540e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model ln(price) ~ ln(carat)\n",
    "\n",
    "\n",
    "\n",
    "# Metrics on the log scale\n",
    "# Residual standard error (s_e) on the log scale\n",
    "# model_ln_ln.scale is the variance of the residuals, so take sqrt\n",
    "s_e_log = np.sqrt(model_ln_ln.scale)\n",
    "\n",
    "# R^2 on the log scale\n",
    "r2_log = model_ln_ln.rsquared\n",
    "\n",
    "\n",
    "# B) Back-transform to price scale\n",
    "pred_ln_price =      # Predicted ln(price)\n",
    "pred_price =          # Predicted price\n",
    "\n",
    "# Residuals on the original price scale\n",
    "resids_price = \n",
    "\n",
    "# Residual standard error (RMSE) on the original scale\n",
    "s_e_price = \n",
    "\n",
    "# R^2 on the original scale\n",
    "price_actual = \n",
    "tss = \n",
    "rss = \n",
    "r2_price = 1 - rss/tss\n",
    "\n",
    "# Print results\n",
    "print(\"Model ln(price) ~ ln(carat):\")\n",
    "print(\" -- On the log scale:\")\n",
    "print(\"    s_e (log scale) =\", s_e_log)\n",
    "print(\"    R^2 (log scale) =\", r2_log, \"\\n\")\n",
    "print(\" -- On the original scale:\")\n",
    "print(\"    s_e (price scale) =\", s_e_price)\n",
    "print(\"    R^2 (price scale) =\", r2_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a823b37c-020c-4427-a3f6-cde76cec08d9",
   "metadata": {},
   "source": [
    "Compute b, the OLS slope coefficients for this new model of ln_price ~ ln_carat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a98c67-2dc8-4ce2-88c3-decad9919825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the slope coefficient (b) for ln_carat\n",
    "b = \n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf03059f-573d-4614-96c1-d62ae516f50e",
   "metadata": {},
   "source": [
    "Interpret b_1, the estimated slope of ln_carat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6ffdca-a2b9-44a4-8e8a-75a1d5fc03c7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe555c89-1605-493c-a679-78fd90595f65",
   "metadata": {},
   "source": [
    "Interpret b_0, the estimated intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b893e9b1-ca3d-4519-a46d-361bce6ceda5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58b7a57e-1f14-4007-9fe4-371b1cc66af0",
   "metadata": {},
   "source": [
    "Create other features ln_x, ln_y, ln_z, ln_depth, ln_table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267d4588-1e0d-4c6b-a697-a78d55460f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new log-transformed features\n",
    "epsilon = 1e-6\n",
    "\n",
    "diamonds_ln_subset['ln_x'] = np.log(diamonds_ln_subset['x'] + epsilon)\n",
    "diamonds_ln_subset['ln_y'] = \n",
    "diamonds_ln_subset['ln_z'] = \n",
    "diamonds_ln_subset['ln_depth'] = \n",
    "diamonds_ln_subset['ln_table'] = \n",
    "\n",
    "# Optionally, display the first few rows of the transformed dataset\n",
    "print(diamonds_ln_subset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e490d4b1-6af6-4349-9d4a-4c0a475fcb11",
   "metadata": {},
   "source": [
    "From now on, we will be modeling ln_price (not raw price) as the prediction target. \n",
    "\n",
    "Create a model (B) of ln_price on ln_carat interacted with clarity and compare its performance with the model (A) ln_price ~ ln_carat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3f706b-ab4b-4120-b1df-fad0740d1e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Model A: ln_price ~ ln_carat\n",
    "model_A = smf.ols('ln_price ~ ln_carat', data=diamonds_ln_subset).fit()\n",
    "r2_A = model_A.rsquared\n",
    "rmse_A = np.sqrt(np.mean(model_A.resid**2))\n",
    "\n",
    "\n",
    "# Model B: ln_price ~ ln_carat * clarity\n",
    "model_B = \n",
    "r2_B = \n",
    "rmse_B = \n",
    "\n",
    "# Print comparison\n",
    "print(\"Model A: ln_price ~ ln_carat\")\n",
    "print(\"  R² =\", r2_A)\n",
    "print(\"  RMSE =\", rmse_A)\n",
    "print(\"\")\n",
    "print(\"Model B: ln_price ~ ln_carat * clarity\")\n",
    "print(\"  R² =\", r2_B)\n",
    "print(\"  RMSE =\", rmse_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f25edb-d762-45a7-b0dc-2f410b079636",
   "metadata": {},
   "source": [
    "Which model does better? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ad8bc2-fbd8-4221-8fc4-c5f495ada798",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d90cd445-eda2-4024-a42a-64704f91ce10",
   "metadata": {},
   "source": [
    "Create a model of (C) ln_price on ln_carat interacted with every categorical feature (clarity, cut and color) and compare its performance with model (B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b7ee54-fb95-4a02-95ff-0ace0e21346e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model C: ln_price ~ ln_carat interacted with clarity, cut, and color\n",
    "model_C =\n",
    "r2_C = \n",
    "rmse_C =\n",
    "\n",
    "print(\"Model C (ln_price ~ ln_carat * C(clarity) + ln_carat * C(cut) + ln_carat * C(color)):\")\n",
    "print(\"  R² =\", r2_C)\n",
    "print(\"  RMSE =\", rmse_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d45db8f-9ab0-4a00-a9bf-3e5138bba11b",
   "metadata": {},
   "source": [
    "Which model does better? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf2400c-1253-433f-89d5-4f3428ddb30d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eafae9a4-3b3a-47c7-a697-c2f551b5aad7",
   "metadata": {},
   "source": [
    "Create a model (D) of ln_price on every continuous feature (logs of carat, x, y, z, depth, table) interacted with every categorical feature (clarity, cut and color) and compare its performance with model (C)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a84d35-0e13-4309-9e1f-237d3c755a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model D: ln_price ~ (ln_carat + ln_x + ln_y + ln_z + ln_depth + ln_table) interacted with (clarity + cut + color)\n",
    "# We include all continuous logs in one group and interact with all three categorical features.\n",
    "# In the formula, '+' combines terms and '*' expands to main effects and interactions.\n",
    "model_D = \n",
    "r2_D = \n",
    "rmse_D = \n",
    "\n",
    "print(\"Model D (ln_price ~ (all continuous logs) * (C(clarity) + C(cut) + C(color))):\")\n",
    "print(\"  R² =\", r2_D)\n",
    "print(\"  RMSE =\", rmse_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115bf067-a482-4cf2-8414-90b837edd49a",
   "metadata": {},
   "source": [
    "Which model does better? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1624acf1-1427-4c13-a045-b89ecc8a6b17",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c00cd590-f632-4e55-b56f-0e21e56fb92a",
   "metadata": {},
   "source": [
    "What is the p of this model D? Compute with code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f740666d-31fd-4ee9-9dba-921ff45a6f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of parameters (p) in the model, including the intercept\n",
    "p = len()\n",
    "print(\"The number of parameters (p) in Model D is:\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd9be68-9c83-429e-a022-c09959b062c0",
   "metadata": {},
   "source": [
    "Create model (E) which is the same as before except create include the raw features interacted with the categorical features and gauge the performance against (D)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6732f66d-9215-417f-8b56-b33c8e9f4d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model E: using raw continuous features\n",
    "# ln_price ~ (carat + x + y + z + depth + table) * (C(clarity) + C(cut) + C(color))\n",
    "model_E = \n",
    "r2_E = \n",
    "rmse_E = n\n",
    "\n",
    "print(\"Model E (using raw continuous features):\")\n",
    "print(\"  R² =\", r2_E)\n",
    "print(\"  RMSE =\", rmse_E)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882de633-6cef-47a6-bbdc-1832dd1553fd",
   "metadata": {},
   "source": [
    "Which model does better? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ac3f43-38bf-4f60-91c0-3ac4dc325fed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "474144d2-86df-4bf9-bf80-39d1586deaae",
   "metadata": {},
   "source": [
    "Create model (F) which is the same as before except also include also third degree polynomials of the continuous features interacted with the categorical features and gauge performance against (E). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c627742-3a83-4850-b60e-d17098d75630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model F: using third degree polynomials of continuous features interacted with categorical features\n",
    "# We add polynomial terms for each continuous variable using I() for raw polynomials.\n",
    "formula_F = (\n",
    "    \"ln_price ~ ((carat + I(carat**2) + I(carat**3)) + \"\n",
    "    \"             (x + I(x**2) + I(x**3)) + \"\n",
    "    \"             (y + I(y**2) + I(y**3)) + \"\n",
    "    \"             (z + I(z**2) + I(z**3)) + \"\n",
    "    \"             (depth + I(depth**2) + I(depth**3)) + \"\n",
    "    \"             (table + I(table**2) + I(table**3))) * \"\n",
    "    \"(C(clarity) + C(cut) + C(color))\"\n",
    ")\n",
    "model_F = \n",
    "r2_F = \n",
    "rmse_F =\n",
    "\n",
    "print(\"Model F (3rd degree polynomials of continuous features):\")\n",
    "print(\"  R² =\", r2_F)\n",
    "print(\"  RMSE =\", rmse_F)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c41b3f4-3231-4089-8270-d3430353f907",
   "metadata": {},
   "source": [
    "Which model does better? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9114fa-72ad-418d-8bf1-93c3791f1866",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "980aae98-fc60-4e5e-959f-ecea6290ea8a",
   "metadata": {},
   "source": [
    "Can you think of any other way to expand the candidate set curlyH? Discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79d69ad-fa85-4b80-af82-362b1bb4a0d3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98c617cc-29c8-457e-b7b9-f5d80c4ffa5c",
   "metadata": {},
   "source": [
    "We should probably assess oos performance now. Sample 2,000 diamonds and use these to create a training set of 1,800 random diamonds and a test set of 200 random diamonds. Define K and do this splitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e515d1-1d3d-44cc-ae24-658032545feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)  # for reproducibility\n",
    "\n",
    "# Define K and sample K diamonds\n",
    "K = \n",
    "diamonds_sample = \n",
    "\n",
    "# Randomly select indices for training set (1,800 out of K) and define test set as the remainder\n",
    "train_indices = \n",
    "train_set = \n",
    "test_set = \n",
    "\n",
    "# Verify sizes\n",
    "print(\"Training set size:\", train_set.shape[0])\n",
    "print(\"Test set size:\", test_set.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f669f2-d758-42d3-b142-d61a1d14c20e",
   "metadata": {},
   "source": [
    "Compute in and out of sample performance for models A-F. Use s_e as the metric (standard error of the residuals). Create a list with keys A, B, ..., F to store these metrics. Remember the performances here will be worse than before since before you're using nearly 52,000 diamonds to build a model and now it's only 1,800! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c326a0-95dc-4cdd-842b-d034880a674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define formulas for models A-F:\n",
    "formulas = {\n",
    "    'A': \"ln_price ~ ln_carat\",\n",
    "    'B': \"ln_price ~ ln_carat * C(clarity)\",\n",
    "    'C': \"ln_price ~ ln_carat * (C(clarity) + C(cut) + C(color))\",\n",
    "    'D': \"ln_price ~ (ln_carat + ln_x + ln_y + ln_z + ln_depth + ln_table) * (C(clarity) + C(cut) + C(color))\",\n",
    "    'E': \"ln_price ~ (carat + x + y + z + depth + table) * (C(clarity) + C(cut) + C(color))\",\n",
    "    'F': \"ln_price ~ ((carat + I(carat**2) + I(carat**3)) + (x + I(x**2) + I(x**3)) + (y + I(y**2) + I(y**3)) + (z + I(z**2) + I(z**3)) + (depth + I(depth**2) + I(depth**3)) + (table + I(table**2) + I(table**3))) * (C(clarity) + C(cut) + C(color))\"\n",
    "}\n",
    "\n",
    "performance = {}\n",
    "\n",
    "# Loop over models A-F\n",
    "for key, formula in formulas.items():\n",
    "    model = smf.ols(formula, data=train_set).fit()\n",
    "    # In-sample performance (RMSE on ln(price))\n",
    "    pred_train = model.predict(train_set)\n",
    "    s_e_in = np.sqrt(np.mean((train_set['ln_price'] - pred_train)**2))\n",
    "    # Out-of-sample performance\n",
    "    pred_test = model.predict(test_set)\n",
    "    s_e_out = np.sqrt(np.mean((test_set['ln_price'] - pred_test)**2))\n",
    "    performance[key] = {'in_sample': s_e_in, 'out_sample': s_e_out}\n",
    "\n",
    "# Convert the performance dictionary to a pandas DataFrame for a tabular view\n",
    "performance_table = pd.DataFrame.from_dict(performance, orient='index')\n",
    "performance_table.index.name = 'Model'\n",
    "print(performance_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fecf87-bffa-4f9a-b5df-99315c71948b",
   "metadata": {},
   "source": [
    "You computed oos metrics only on n_* = 200 diamonds. What problem(s) do you expect in these oos metrics?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bae6f61-d2d3-4425-9459-ba3b177adc53",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a45886c4-f25d-4821-a3a6-1848d9518eb9",
   "metadata": {},
   "source": [
    "To do the K-fold cross validation we need to get the splits right and crossing is hard. We've developed code for this already in a previous lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765a8d03-1f05-4655-836b-5c0160bf53d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "n =   # total number of observations\n",
    "K =      # number of folds (you can change this as needed)\n",
    "\n",
    "# Create a KFold object with shuffling for randomness and a fixed random_state for reproducibility\n",
    "\n",
    "# Create an empty array to store fold assignments\n",
    "\n",
    "\n",
    "# Loop over the splits and assign fold numbers\n",
    "for fold, (_, test_index) in enumerate(kf.split(np.arange(n)), start=1):\n",
    "    folds_vec[test_index] = fold\n",
    "\n",
    "# Print the first 200 fold assignments\n",
    "print(folds_vec[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a1d05b-09ab-4368-aa21-8d7467ed8e3a",
   "metadata": {},
   "source": [
    "Do the K-fold cross validation for model F and compute the overall s_e and s_s_e. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231b79b3-b71b-4e8f-88a0-29c33a4b721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "# Assume diamonds_ln_subset is a preprocessed DataFrame with log variables already created for ln_price\n",
    "# If not, make sure to compute ln_price = np.log(price) for each row.\n",
    "\n",
    "# Sample K = 2000 diamonds and split into training (1,800) and test (200) sets\n",
    "K = 2000\n",
    "diamonds_sample = \n",
    "train_indices = \n",
    "train_set =\n",
    "test_set = \n",
    "\n",
    "print(\"Training set size:\", train_set.shape[0])\n",
    "print(\"Test set size:\", test_set.shape[0])\n",
    "\n",
    "# Set up K-fold cross validation on the training set using scikit-learn's KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "# Define Model F formula:\n",
    "formula_F = (\n",
    "    \"ln_price ~ ((carat + I(carat**2) + I(carat**3)) + \"\n",
    "    \"(x + I(x**2) + I(x**3)) + \"\n",
    "    \"(y + I(y**2) + I(y**3)) + \"\n",
    "    \"(z + I(z**2) + I(z**3)) + \"\n",
    "    \"(depth + I(depth**2) + I(depth**3)) + \"\n",
    "    \"(table + I(table**2) + I(table**3))) * (C(clarity) + C(cut) + C(color))\"\n",
    ")\n",
    "\n",
    "fold_rmse = []\n",
    "total_SSE = \n",
    "total_n = \n",
    "\n",
    "# Perform cross-validation over the training set:\n",
    "for train_idx, val_idx in kf.split(train_set):\n",
    "    train_fold = \n",
    "    val_fold = \n",
    "    \n",
    "    model_F_cv = \n",
    "    pred = \n",
    "    resid = \n",
    "    s_e_fold = \n",
    "    fold_rmse.append()\n",
    "    \n",
    "    total_SSE +=\n",
    "    total_n += \n",
    "\n",
    "overall_s_e = np.sqrt(total_SSE / total_n)\n",
    "s_s_e = np.std(fold_rmse, ddof=1)\n",
    "\n",
    "# Now, fit Model F on the entire training set and compute out-of-sample performance on the test set:\n",
    "model_F_final = \n",
    "pred_test = \n",
    "oos_s_e = \n",
    "\n",
    "# Create a performance table as a DataFrame:\n",
    "performance_table = pd.DataFrame({\n",
    "    \"Model\": [\"F\"],\n",
    "    \"Overall_s_e\": [overall_s_e],\n",
    "    \"s_s_e\": [s_s_e],\n",
    "    \"OOS_s_e\": [oos_s_e]\n",
    "})\n",
    "print(performance_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cded1ef8-7fb2-448f-a454-eddc364ef242",
   "metadata": {},
   "source": [
    "Does K-fold CV help reduce variance in the oos s_e? Discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a61a06-6336-4d9d-8456-654e47d10710",
   "metadata": {},
   "source": [
    "K-fold cross validation helps reduce the variance in the out-of-sample sₑ estimate because it averages performance over multiple different splits of the training data. Instead of relying on one arbitrary partition of data, K-fold CV trains and evaluates the model K times on different subsets, which smooths out the influence of any one particularly “easy” or “hard” split. However, while this approach reduces variance compared to a single train–test split, some variability may still remain, especially if the overall sample size is small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40487d6-6f6f-44b4-a8b5-8febeed47bd7",
   "metadata": {},
   "source": [
    "Imagine using the entire rest of the dataset besides the 2,000 training observations divvied up into slices of 200. Measure the oos error for each slice on Model F in a vector `s_e_s_F` and compute the `s_s_e_F` and also plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2984e293-ad7b-49d9-8ee4-372233db1031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "# Sample K = 2000 diamonds from the full dataset without resetting the index.\n",
    "K = \n",
    "diamonds_sample = \n",
    "\n",
    "# Randomly select indices for the training set (1,800 out of K) and define test set as the remainder\n",
    "train_indices = np.random.choice(diamonds_sample.index, size=1800, replace=False)\n",
    "train_set = diamonds_sample.loc[train_indices].reset_index(drop=True)\n",
    "test_set = diamonds_sample.drop(train_indices).reset_index(drop=True)\n",
    "\n",
    "print(\"Training set size:\", train_set.shape[0])\n",
    "print(\"Test set size:\", test_set.shape[0])\n",
    "\n",
    "# Define the rest set as all observations not in diamonds_sample\n",
    "rest_set = diamonds_ln_subset.drop(diamonds_sample.index).reset_index(drop=True)\n",
    "\n",
    "# Split the rest_set into slices of 200 observations each (using only complete slices)\n",
    "num_slices = len(rest_set) // 200\n",
    "rest_set_subset = rest_set.iloc[:num_slices * 200]\n",
    "slices = np.array_split(rest_set_subset, num_slices)\n",
    "\n",
    "# Fit Model F on the training set.\n",
    "# Model F: ln_price ~ ((carat + I(carat**2) + I(carat**3)) + (x + I(x**2) + I(x**3)) +\n",
    "#                      (y + I(y**2) + I(y**3)) + (z + I(z**2) + I(z**3)) +\n",
    "#                      (depth + I(depth**2) + I(depth**3)) + (table + I(table**2) + I(table**3))) *\n",
    "#                      (C(clarity) + C(cut) + C(color))\n",
    "formula_F = (\n",
    "    \"ln_price ~ ((carat + I(carat**2) + I(carat**3)) + \"\n",
    "    \"(x + I(x**2) + I(x**3)) + \"\n",
    "    \"(y + I(y**2) + I(y**3)) + \"\n",
    "    \"(z + I(z**2) + I(z**3)) + \"\n",
    "    \"(depth + I(depth**2) + I(depth**3)) + \"\n",
    "    \"(table + I(table**2) + I(table**3))) * (C(clarity) + C(cut) + C(color))\"\n",
    ")\n",
    "model_F_final = smf.ols(formula_F, data=train_set).fit()\n",
    "\n",
    "# Compute out-of-sample RMSE (s_e) for each slice\n",
    "s_e_s_F = []\n",
    "for slice_df in slices:\n",
    "    pred = model_F_final.predict(slice_df)\n",
    "    resid = slice_df['ln_price'] - pred\n",
    "    rmse = np.sqrt(np.mean(resid**2))\n",
    "    s_e_s_F.append(rmse)\n",
    "\n",
    "s_e_s_F = np.array(s_e_s_F)\n",
    "s_s_e_F = np.std(s_e_s_F, ddof=1)\n",
    "\n",
    "print(\"s_e_s_F (RMSE for each slice):\", s_e_s_F)\n",
    "print(\"s_s_e_F (Std. dev. of slice RMSE):\", s_s_e_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f45ff6-20ed-4f05-ab6c-b5d993e534c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import ggplot, aes, geom_histogram, labs, theme_minimal, scale_x_log10\n",
    "\n",
    "df_plot = pd.DataFrame({'s_e_s_F': s_e_s_F})\n",
    "\n",
    "p = (ggplot(df_plot, aes(x='s_e_s_F'))\n",
    "     + geom_histogram(binwidth=0.01, fill=\"blue\", color=\"black\", alpha=0.7)\n",
    "     + labs(title=\"Histogram of OOS s_e for Model F (log scale)\", \n",
    "            x=\"s_e (RMSE, log scale)\", \n",
    "            y=\"Count\")\n",
    "     + scale_x_log10()\n",
    "     + theme_minimal())\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96d0887-3713-4d14-9e89-42a54adcaebc",
   "metadata": {},
   "source": [
    "#Rcpp and optimizing R\n",
    "\n",
    "Write a function `dot_product_py` in python that takes in two vectors `v1` and `v2` and returns their dot product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f59d52c-ae56-44c2-854c-b77f18bdb0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product_py(v1, v2):\n",
    "    \n",
    "\n",
    "# Example usage:\n",
    "v1 = \n",
    "v2 = \n",
    "print()  # Output should be 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f5f458-4b33-4762-af16-cb9183b46ead",
   "metadata": {},
   "source": [
    "Write a function `dot_product_cpp` in C++ and make sure it compiles. First uncomment the cell below and install cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0ae781-7ca1-41e4-882f-b9382de16f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399d0518-b817-4579-8840-0e9f68ebface",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d2cdad-141c-47df-88d6-7ea3ee3e0812",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython --cplus\n",
    "# dot_product_cython.pyx\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "\n",
    "def dot_product_cython(np.ndarray[np.double_t, ndim=1] v1, np.ndarray[np.double_t, ndim=1] v2):\n",
    "    \"\"\"\n",
    "    Compute the dot product of two 1D NumPy arrays of type double.\n",
    "    \"\"\"\n",
    "    cdef Py_ssize_t n = v1.shape[0]\n",
    "    if v2.shape[0] != n:\n",
    "        raise ValueError(\"Vectors must have the same length\")\n",
    "    cdef Py_ssize_t i\n",
    "    cdef double result = 0.0\n",
    "    for i in range(n):\n",
    "        result += v1[i] * v2[i]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85bcb12-6904-49f7-b299-7b95adc61ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "v1 = np.array([1.0, 2.0, 3.0], dtype=np.float64)\n",
    "v2 = np.array([4.0, 5.0, 6.0], dtype=np.float64)\n",
    "\n",
    "result = dot_product_cython(v1, v2)\n",
    "print(\"Dot product (Cython):\", result)  # Expected output: 32.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef586584-0826-41bc-a84c-8619d2886f4e",
   "metadata": {},
   "source": [
    "Create two vectors of standard normal realizations with length `n=1e6` and test the different in speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6789276-dfce-409e-8744-e968dfda7a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import timeit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46654483-e3b0-45c4-a6ae-4b3f4e2f49e6",
   "metadata": {},
   "source": [
    "Implement the Gram Schmidt routine as a C++ function `gram_schmidt_cython`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a47bcec-1137-4557-8ee8-4536e580e952",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython --cplus\n",
    "# cython: language_level=3\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "cimport cython\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "def gram_schmidt_cython(np.ndarray[np.double_t, ndim=2] A):\n",
    "    \"\"\"\n",
    "    Perform classical Gram-Schmidt orthonormalization on the columns of matrix A.\n",
    "    Returns an orthonormal matrix Q (of shape (m, n)) with the same dimensions as A.\n",
    "    \n",
    "    Parameters:\n",
    "      A : np.ndarray[np.double_t, ndim=2]\n",
    "          Input matrix (m x n), whose columns will be orthonormalized.\n",
    "    \n",
    "    Returns:\n",
    "      Q : np.ndarray[np.double_t, ndim=2]\n",
    "          Orthonormalized matrix.\n",
    "    \"\"\"\n",
    "    cdef int m = A.shape[0]\n",
    "    cdef int n = A.shape[1]\n",
    "    cdef int i, j, k\n",
    "    cdef double norm_val, dot_val\n",
    "    cdef np.ndarray[np.double_t, ndim=2] Q = np.empty((m, n), dtype=np.double)\n",
    "    \n",
    "    # Copy A into Q\n",
    "    for j in range(n):\n",
    "        for i in range(m):\n",
    "            Q[i, j] = A[i, j]\n",
    "    \n",
    "    # Gram-Schmidt process\n",
    "    for j in range(n):\n",
    "        # Compute the norm of the j-th column\n",
    "        norm_val = 0.0\n",
    "        for i in range(m):\n",
    "            norm_val += Q[i, j] * Q[i, j]\n",
    "        norm_val = norm_val ** 0.5\n",
    "        if norm_val == 0:\n",
    "            raise ValueError(\"Zero column encountered in input matrix\")\n",
    "        # Normalize the j-th column\n",
    "        for i in range(m):\n",
    "            Q[i, j] /= norm_val\n",
    "        # Orthogonalize subsequent columns against the j-th column\n",
    "        for k in range(j + 1, n):\n",
    "            dot_val = 0.0\n",
    "            for i in range(m):\n",
    "                dot_val += Q[i, j] * Q[i, k]\n",
    "            for i in range(m):\n",
    "                Q[i, k] -= dot_val * Q[i, j]\n",
    "                \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0735a3f6-93a3-47fd-9fdb-6d665d4f3b56",
   "metadata": {},
   "source": [
    "Here is the implementation in taken from lab 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b98fd34-e1f6-44fa-bebd-55bf9d879ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gram_schmidt_R(X):\n",
    "    \"\"\"\n",
    "    Perform classical Gram-Schmidt orthonormalization on the columns of matrix X.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy.ndarray, shape (m, n)\n",
    "        Input matrix whose columns will be orthonormalized.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Q : numpy.ndarray, shape (m, n)\n",
    "        Orthonormal matrix whose columns are the orthonormalized version of X's columns.\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    # Create an empty matrix to store the orthogonalized vectors\n",
    "    V = np.empty((m, n), dtype=X.dtype)\n",
    "    # Set the first column of V as the first column of X\n",
    "    V[:, 0] = X[:, 0]\n",
    "    \n",
    "    # For each subsequent column j\n",
    "    for j in range(1, n):\n",
    "        # Start with the j-th column of X\n",
    "        V[:, j] = X[:, j]\n",
    "        # Subtract the projection of X[:, j] on each previously computed v_k\n",
    "        for k in range(j):\n",
    "            v_k = V[:, k]\n",
    "            # Compute the projection coefficient: (v_k^T X[:, j])/(||v_k||^2)\n",
    "            proj_coeff = np.dot(v_k, X[:, j]) / np.sum(v_k**2)\n",
    "            # Subtract the projection\n",
    "            V[:, j] -= proj_coeff * v_k\n",
    "    \n",
    "    # Normalize each column of V to obtain Q\n",
    "    Q = np.empty((m, n), dtype=X.dtype)\n",
    "    for j in range(n):\n",
    "        norm = np.sqrt(np.sum(V[:, j]**2))\n",
    "        Q[:, j] = V[:, j] / norm\n",
    "    \n",
    "    return Q\n",
    "\n",
    "\n",
    "# Example ... Create a random 5x3 matrix\n",
    "X = np.random.randn(5, 3)\n",
    "Q = gram_schmidt_R(X)\n",
    "print(\"Input matrix X:\")\n",
    "print(X)\n",
    "print(\"\\nOrthonormal matrix Q:\")\n",
    "print(Q)\n",
    "# Verify orthonormality: Q^T Q should be close to the identity matrix.\n",
    "print(\"\\nQ^T Q:\")\n",
    "print(np.dot(Q.T, Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa43a9ea-f193-420f-ac08-ce0d6fc3ec4b",
   "metadata": {},
   "source": [
    "Now let's see how much faster C++ is by running it on the boston housing data design matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24ee290-6973-418d-975a-c5290a58fbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import timeit\n",
    "\n",
    "# ------------------------------\n",
    "# Step 1: Load and process the Boston housing data\n",
    "# ------------------------------\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=r\"\\s+\", skiprows=22, header=None)\n",
    "\n",
    "# Combine the two halves: even rows are predictors, odd rows give response and extra columns\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]\n",
    "col_names = [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\",\n",
    "             \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\", \"MEDV\"]\n",
    "df = pd.DataFrame(np.hstack([data, target.reshape(-1, 1)]), columns=col_names)\n",
    "df = df.astype(float)\n",
    "\n",
    "# ------------------------------\n",
    "# Step 2: Create design matrix without patsy (using statsmodels)\n",
    "# ------------------------------\n",
    "X = df.drop(\"MEDV\", axis=1)\n",
    "X = sm.add_constant(X)\n",
    "y = df[\"MEDV\"]\n",
    "\n",
    "# For benchmarking, convert X to a NumPy array\n",
    "X_np = np.asarray(X)\n",
    "\n",
    "# ------------------------------\n",
    "# Step 4: Assume gram_schmidt_cython is already compiled (fallback to pure Python if not)\n",
    "# ------------------------------\n",
    "try:\n",
    "    gram_schmidt_cython\n",
    "except NameError:\n",
    "    print(\"Warning: gram_schmidt_cython is not defined; using pure Python version instead.\")\n",
    "    gram_schmidt_cython = gram_schmidt_R\n",
    "\n",
    "# ------------------------------\n",
    "# Step 5: Benchmark both implementations using timeit (10 runs each)\n",
    "# ------------------------------\n",
    "py_time = timeit.timeit(lambda: gram_schmidt_R(X_np), number=10)\n",
    "cy_time = timeit.timeit(lambda: gram_schmidt_cython(X_np), number=10)\n",
    "\n",
    "print(\"Pure Python Gram-Schmidt time over 10 runs:\", py_time)\n",
    "print(\"Cython Gram-Schmidt time over 10 runs:\", cy_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6eed13-bb9e-4e20-90f3-cda9a899673a",
   "metadata": {},
   "source": [
    "Extra Credit (+5): Create a variable `n` to be 10 and a vaiable `Nvec` to be 100 initially. Create a random vector via `np.random.randn` `Nvec` times and load it into a `Nvec` x `n` dimensional matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5300172b-ea90-43ce-a46b-9efff01886d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4238364a-ede0-4986-980a-970df50d5003",
   "metadata": {},
   "source": [
    "Extra Credit (+5): Write a function `all_angles` that measures the angle between each of the pairs of vectors. You should measure the vector on a scale of 0 to 180 degrees with negative angles coerced to be positive. Then plot the density of these angles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3dca6a-0373-4b05-9f8d-a8b2607a90ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87577dbd-7c23-4ae9-9ef3-bfc9f892b518",
   "metadata": {},
   "source": [
    "Write an Rcpp function `all_angles_cpp` that does the same thing. Use an IDE if you want, but write it below in-line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a11e4b-fb9a-4b69-9fd7-3453b940a38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython --cplus\n",
    "# cython: language_level=3\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "from libc.math cimport acos, sqrt, M_PI\n",
    "\n",
    "def all_angles_cy(np.ndarray[np.double_t, ndim=2] X):\n",
    "    \"\"\"\n",
    "    Compute the angle (in degrees) between each pair of vectors (rows of X)\n",
    "    using the classical dot product formula. Returns an n x n matrix of angles.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray[np.double_t, ndim=2]\n",
    "         Input matrix with shape (n, m), where each row is a vector.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    angle_matrix : np.ndarray[np.double_t, ndim=2]\n",
    "         Matrix of angles (in degrees) between the vectors.\n",
    "    \"\"\"\n",
    "    cdef int n = X.shape[0]\n",
    "    cdef int m = X.shape[1]\n",
    "    cdef int i, j, k\n",
    "    cdef double dot_prod, norm_i, norm_j, cos_val, angle_rad, angle_deg\n",
    "    cdef np.ndarray[np.double_t, ndim=2] angle_matrix = np.empty((n, n), dtype=np.double)\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            dot_prod = 0.0\n",
    "            norm_i = 0.0\n",
    "            norm_j = 0.0\n",
    "            for k in range(m):\n",
    "                dot_prod += X[i, k] * X[j, k]\n",
    "                norm_i += X[i, k] * X[i, k]\n",
    "                norm_j += X[j, k] * X[j, k]\n",
    "            norm_i = sqrt(norm_i)\n",
    "            norm_j = sqrt(norm_j)\n",
    "            cos_val = dot_prod / (norm_i * norm_j)\n",
    "            if (cos_val > 1.0):\n",
    "                cos_val = 1.0\n",
    "            elif (cos_val < -1.0):\n",
    "                cos_val = -1.0\n",
    "            angle_rad = acos(cos_val)\n",
    "            angle_deg = angle_rad * 180.0 / M_PI\n",
    "            angle_matrix[i, j] = angle_deg\n",
    "            angle_matrix[j, i] = angle_deg\n",
    "    return angle_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c883356-17bc-4e20-ac09-fb23d9c419c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a small example matrix where each row is a vector.\n",
    "X_example = np.array([[1.0, 0.0],\n",
    "                      [0.0, 1.0],\n",
    "                      [1.0, 1.0]])\n",
    "print(all_angles_cy(X_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772425e7-3e9f-4e08-87b3-9c2cc5fcb06a",
   "metadata": {},
   "source": [
    "Extra Credit (+5): Test the time difference between these functions for `n = 1000` and `Nvec = 100, 500, 1000, 5000` using the package `microbenchmark`.  Store the results in a matrix with rows representing `Nvec` and two columns for base R and Rcpp."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
