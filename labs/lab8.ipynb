{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "497991d0-557d-42c4-a202-2a069446753d",
   "metadata": {},
   "source": [
    "## Course Assignment Instructions\n",
    "You should have Python (version 3.8 or later) and Jupyter Notebook installed to complete this assignment. You will write code in the empty cell/cells below the problem. While most of this will be a programming assignment, some questions will ask you to \"write a few sentences\" in markdown cells. \n",
    "\n",
    "Submission Instructions:\n",
    "\n",
    "Create a labs directory in your personal class repository (e.g., located in your home directory)\n",
    "Clone the class repository\n",
    "Copy this Jupyter notebook file (.ipynb) into your repo/labs directory\n",
    "Make your edits, commit changes, and push to your repository\n",
    "All submissions must be pushed before the due date to avoid late penalties. \n",
    "\n",
    "Labs are graded out of a 100 pts. Each day late is -10. For a max penalty of -50 after 5 days. From there you may submit the lab anytime before the semester ends for a max score of 50.  \n",
    "\n",
    "Lab 8 is due on 4/28/25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff97c825-1d37-446d-91e5-d231f2083b8c",
   "metadata": {},
   "source": [
    "## Model Selection with Three Splits: Select from M models\n",
    "\n",
    "We employ the diamonds dataset and specify M models nested from simple to more complex. We store the models as strings in a list (i.e. a hashset) ... Create log and polynomial transformations of the following features (carat, x, y, z, depth, and table). In order to use the formulas with logs we need to eliminate rows with zeros in those measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d76806-4399-4759-8a56-9b6ba16eab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "from plotnine.data import diamonds\n",
    "\n",
    "# Load the diamonds dataset and create a copy\n",
    "diamonds = diamonds.copy()\n",
    "\n",
    "# Filter to remove rows with non-positive values in the specified columns\n",
    "diamonds_cleaned = diamonds.loc[\n",
    "    (diamonds[\"carat\"] > 0) &\n",
    "    \n",
    "   ].copy()\n",
    "\n",
    "# Add polynomial (squared) features\n",
    "diamonds_cleaned['carat_sq'] = diamonds_cleaned['carat'] ** 2\n",
    "diamonds_cleaned['x_sq'] = \n",
    "diamonds_cleaned['y_sq'] =\n",
    "diamonds_cleaned['z_sq'] = \n",
    "diamonds_cleaned['depth_sq'] = \n",
    "diamonds_cleaned['table_sq'] = \n",
    "\n",
    "# Add log-transformed features (add small constant to avoid log(0))\n",
    "epsilon = 1e-6\n",
    "diamonds_cleaned['log_carat'] = np.log(diamonds_cleaned['carat'] + epsilon)\n",
    "diamonds_cleaned['log_x'] = \n",
    "diamonds_cleaned['log_y'] = \n",
    "diamonds_cleaned['log_z'] = \n",
    "diamonds_cleaned['log_depth'] = \n",
    "diamonds_cleaned['log_table'] = \n",
    "\n",
    "# Model formulas (now referencing the precomputed columns)\n",
    "model_formulas = [\n",
    "    \"carat\",\n",
    "    \"carat + cut\",\n",
    "    \"carat + cut + color\",\n",
    "    \"carat + cut + color + clarity\",\n",
    "    \"carat + cut + color + clarity + x + y + z\",\n",
    "    \"carat + cut + color + clarity + x + y + z + depth\",\n",
    "    \"carat + cut + color + clarity + x + y + z + depth + table\",\n",
    "    \"carat * (cut + color + clarity) + x + y + z + depth + table\",\n",
    "    \"(carat + x + y + z) * (cut + color + clarity) + depth + table\",\n",
    "    \"(carat + x + y + z + depth + table) * (cut + color + clarity)\",\n",
    "    \"(carat_sq + x + y + z + depth + table) * (cut + color + clarity)\",\n",
    "    \"(carat_sq + x_sq + y_sq + z_sq + depth + table) * (cut + color + clarity)\",\n",
    "    \"(carat_sq + x_sq + y_sq + z_sq + depth_sq + table_sq) * (cut + color + clarity)\",\n",
    "    \"(carat_sq + x_sq + y_sq + z_sq + depth_sq + table_sq + log_carat + log_x + log_y + log_z) * (cut + color + clarity)\",\n",
    "    \"(carat_sq + x_sq + y_sq + z_sq + depth_sq + table_sq + log_carat + log_x + log_y + log_z + log_depth) * (cut + color + clarity)\",\n",
    "    \"(carat_sq + x_sq + y_sq + z_sq + depth_sq + table_sq + log_carat + log_x + log_y + log_z + log_depth + log_table) * (cut + color + clarity)\",\n",
    "    \"(carat_sq + x_sq + y_sq + z_sq + depth_sq + table_sq + log_carat + log_x + log_y + log_z + log_depth + log_table) * (cut + color + clarity + carat_sq + x_sq + y_sq + z_sq + depth_sq + table_sq + log_carat + log_x + log_y + log_z + log_depth + log_table)\"\n",
    "]\n",
    "\n",
    "# Prefix with 'price ~' for statsmodels compatibility\n",
    "model_formulas = [f\"price ~ {f}\" for f in model_formulas]\n",
    "\n",
    "# Number of formulas\n",
    "M = len(model_formulas)\n",
    "print(f\"Total number of model formulas: {}\")\n",
    "\n",
    "# Preview\n",
    "for i, formula in enumerate(model_formulas):\n",
    "    print(f\"{i+1}: {formula}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07544540-3e75-4b8e-a0f6-1262cd8a447e",
   "metadata": {},
   "source": [
    "Split the data into train, select and test. Each set should have 1/3 of the total data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e679024-6c61-4648-942f-125e92da2d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# First split: ~1/3 (train), ~2/3 (temp)\n",
    "\n",
    "\n",
    "# Second split: from temp, split half (~1/3 overall) for select and half (~1/3 overall) for test\n",
    "\n",
    "\n",
    "print(f\"Train size:  {len()}\")\n",
    "print(f\"Select size: {len()}\")\n",
    "print(f\"Test size:   {len()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a17ca49-51d9-4c0d-8498-638709375336",
   "metadata": {},
   "source": [
    "Find the oosRMSE on the select set for each model. Save the number of df in each model while you're doing this as we'll need it for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b80980-eef8-4d71-9286-4ae447333408",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "\n",
    "for i, formula in enumerate(model_formulas, start=1):\n",
    "    #Fit on train\n",
    "    \n",
    "    #Predict on select\n",
    "    preds = \n",
    "    \n",
    "    #Compute oosRMSE\n",
    "    actual = diamonds_select['price']\n",
    "    rmse = \n",
    "    \n",
    "    #Degrees of freedom\n",
    "    #number of parameters\n",
    "    df_params = len(model.params)\n",
    "\n",
    "    #Store results\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results_list)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f25184-dbfe-4da0-b3a8-c0cb1a7a166b",
   "metadata": {},
   "source": [
    "Plot the oosRMSE by model complexity (df in model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae54460-765f-476b-abff-b8810af8d945",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import ggplot, aes, geom_point, geom_line, theme_bw, labs\n",
    "\n",
    "\n",
    "plot = (ggplot(, aes(x='df', y='oosRMSE')) +\n",
    "     geom_point() + \n",
    "     geom_line() + \n",
    "     labs(title=\"Out-of-Sample RMSE by Model Complexity\", x=\"Degrees of Freedom (df)\", y=\"Out-of-Sample RMSE\") +\n",
    "     theme_bw())\n",
    "\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06922956-adf0-4844-95fa-1c169b712c0a",
   "metadata": {},
   "source": [
    "Select the best model by oosRMSE and find its oosRMSE on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f34647-cb9d-44f3-a03e-ace40599214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify best model by oosRMSE\n",
    "best_row = \n",
    "best_formula = \n",
    "\n",
    "print(\"Best model row:\\n\", )\n",
    "print(\"Best model formula:\", )\n",
    "\n",
    "#Fit the best model on train\n",
    "best_model = smf.ols(formula=best_formula, data=diamonds_train).fit()\n",
    "\n",
    "#Evaluate RMSE on the test set\n",
    "preds_test = \n",
    "test_actual = \n",
    "test_oosRMSE = \n",
    "\n",
    "print(\"oosRMSE on the test set:\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4d3246-7432-4cbe-9596-f777d942f27a",
   "metadata": {},
   "source": [
    "Did we overfit the select set? Discuss why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3991d12-6788-4205-b474-12215acb3c7f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e34cbe1-1eab-456d-b65d-c562d1044a90",
   "metadata": {},
   "source": [
    "Create the final model object `g_final`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec4be25-bcf2-4e19-bbd0-40922fd4edbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify best formula\n",
    "best_row = \n",
    "best_formula = \n",
    "print(\"Best formula:\", )\n",
    "\n",
    "#Combine train and select data\n",
    "diamonds_train_select = \n",
    "\n",
    "#Fit final model on train+select\n",
    "g_final = smf.ols(formula=best_formula, data=diamonds_train_select).fit()\n",
    "\n",
    "#Evaluate on test\n",
    "test_preds = \n",
    "test_actual =\n",
    "test_oosRMSE = \n",
    "\n",
    "print(\"Final model trained on train+select data.\")\n",
    "print(\"oosRMSE on Test Set:\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae476a12-0e4b-42dc-a9e5-f51a0d281b64",
   "metadata": {},
   "source": [
    "## Model Selection with Three Splits: Hyperparameter selection\n",
    "\n",
    "We will use an algorithm that I historically taught in 324W but now moved to 343 so I can teach it more deeply using the Bayesian topics from 341. The regression algorithm is called \"ridge\" and it involves solving for the slope vector via:\n",
    "\n",
    "b_ridge := (X^T X + lambda I_(p+1))^-1 X^T y\n",
    "\n",
    "Note how if lambda = 0, this is the same algorithm as OLS. If lambda becomes very large then b_ridge is pushed towards all zeroes. So ridge is good at weighting only features that matter.\n",
    "\n",
    "However, lambda is a hyperparameter >= 0 that needs to be selected.\n",
    "\n",
    "We will work with the boston housing dataset except we will add 250 garbage features consisting of iid N(0,1) realizations. We will also standardize the columns so they're all xbar = 0 and s_x = 1. This is shown to be important in 343."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36571395-7310-4fda-bd16-eb73a8abc38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "#Read the Boston Housing CSV (exported from R)\n",
    "df = \n",
    "\n",
    "#y is the response\n",
    "y = \n",
    "\n",
    "#X_data: mimic model.matrix(medv ~ ., MASS::Boston):\n",
    "# Drop 'medv' and add an intercept column\n",
    "X_data = \n",
    "X_data_with_const \n",
    "\n",
    "#Add 250 garbage features\n",
    "np.random.seed(1)\n",
    "n = \n",
    "p_garbage =\n",
    "garbage_matrix = \n",
    "\n",
    "# Combine the real predictors + garbage into a single DataFrame\n",
    "df_X = \n",
    "\n",
    "#Standardize each column: (x_j - mean_j)/sd_j, matching R's default sample sd (ddof=1)\n",
    "means = \n",
    "stds  = \n",
    "df_X_std = \n",
    "\n",
    "#The first column is our intercept, but standardizing sets it to 0, so reset it to 1\n",
    "df_X_std.iloc[:, 0] = 1.0\n",
    "\n",
    "#Name the columns\n",
    "orig_columns = [\"Intercept\"] + list(X_data.columns)\n",
    "garb_columns = [f\"garb_{i+1}\" for i in range(p_garbage)]\n",
    "df_X_std.columns = orig_columns + garb_columns\n",
    "\n",
    "# 7) df_X_final is now your final DataFrame of predictors\n",
    "df_X_final = df_X_std.copy()\n",
    "\n",
    "print(\"df_X_final shape:\", )\n",
    "print(\"First 5 rows:\\n\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e271344-b4c7-4b16-b8bb-25ddfe9d9d4c",
   "metadata": {},
   "source": [
    "Now we split it into 300 train, 100 select and 106 test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589d0d33-db95-4db8-8ee3-c3c8c26fc310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split: 300 for train, remainder (206) for temp\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "\n",
    ")\n",
    "\n",
    "#From the remaining 206, split out 100 for select and 106 for test\n",
    "X_select, X_test, y_select, y_test = train_test_split(\n",
    "   \n",
    ")\n",
    "\n",
    "#Print shapes to verify (should be 300 / 100 / 106)\n",
    "print(\"Train set X:\", , \"y:\",)\n",
    "print(\"Select set X:\", , \"y:\",)\n",
    "print(\"Test set X:  \", ,  \"y:\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a47f5fe-6c70-4365-8a5e-8b978e6f7548",
   "metadata": {},
   "source": [
    "We now create a grid of M = 200 models indexed by lambda. The lowest lambda should be zero (which is OLS) and the highest lambda can be 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29593ef3-a3c6-43c9-82d4-27e1ffb3a150",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 200\n",
    "lambda_grid = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc0c65b-55a7-4a16-acd4-094e20d73aea",
   "metadata": {},
   "source": [
    "Now find the oosRMSE on the select set on all models each with their own lambda value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c768248-a24a-4343-801e-dce9fe4fcac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "M = len(lambda_grid)\n",
    "oosRMSE_list = []\n",
    "\n",
    "for lam in lambda_grid:\n",
    "    #Fit a Ridge regression model with alpha=lamba\n",
    "    #Note:alpha here corresponds to lambda in R\n",
    "    #Note: If your X_train does NOT have an intercept column, set fit_intercept=True, else fit_intercept=False\n",
    "    ridge_model = \n",
    "    ridge_model.fit()\n",
    "    \n",
    "    #Predict on the select set\n",
    "    preds = \n",
    "    \n",
    "    #Compute out-of-sample RMSE\n",
    "    mse = \n",
    "    rmse = \n",
    "    \n",
    "\n",
    "oosRMSE_array = np.array()\n",
    "print(oosRMSE_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f611532-7c0e-4fae-8920-e2e95ca1b8bb",
   "metadata": {},
   "source": [
    "Plot the oosRMSE by the value of lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c88e063-7ffa-4b6b-b208-5c6779d40291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from plotnine import ggplot, aes, geom_point, geom_line, labs, theme_bw\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'lambda': \n",
    "    'oosRMSE': \n",
    "})\n",
    "\n",
    "plot = (ggplot(results_df, aes(x='lambda', y='oosRMSE'))\n",
    "        + geom_point()\n",
    "        + geom_line()\n",
    "        + labs(title=\"Out-of-sample RMSE vs. Lambda\", x=\"Lambda\", y=\"OOS RMSE\")\n",
    "        + theme_bw())\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d85f558-a490-4687-9477-8aba992a5bb2",
   "metadata": {},
   "source": [
    "Select the model with the best oosRMSE on the select set and find its oosRMSE on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f217db6-ba07-48c0-bc17-db064acbe21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify the best lambda (lowest oosRMSE on the select set)\n",
    "best_idx = \n",
    "best_lambda = \n",
    "print(\"Best lambda:\", )\n",
    "\n",
    "#Refit on train set with that lambda\n",
    "ridge_best = \n",
    "ridge_best.fit()\n",
    "\n",
    "#Evaluate on the test set\n",
    "preds_test = ridge_best.predict()\n",
    "test_oosRMSE = np.sqrt()\n",
    "print(\"Test-set RMSE:\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4ca216-0f5c-4b03-aa9e-17545372d36c",
   "metadata": {},
   "source": [
    "Create the final model object `g_final`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2539dd-8fc0-41c6-8b01-34a3484937d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X_train to DataFrame if needed (preserve columns if they exist, otherwise create generic ones)\n",
    "if not isinstance(X_train, pd.DataFrame):\n",
    "    X_train = pd.DataFrame(X_train, columns=[f\"X{i}\" for i in range(X_train.shape[1])])\n",
    "if not isinstance(X_select, pd.DataFrame):\n",
    "    X_select = pd.DataFrame(X_select, columns=X_train.columns)\n",
    "if not isinstance(X_test, pd.DataFrame):\n",
    "    X_test = pd.DataFrame(X_test, columns=X_train.columns)\n",
    "\n",
    "#Convert y_train, y_select, and y_test to Series if needed\n",
    "if not isinstance(y_train, pd.Series):\n",
    "    y_train = pd.Series(y_train, name=\"y\")\n",
    "if not isinstance(y_select, pd.Series):\n",
    "    y_select = pd.Series(y_select, name=\"y\")\n",
    "if not isinstance(y_test, pd.Series):\n",
    "    y_test = pd.Series(y_test, name=\"y\")\n",
    "\n",
    "#Combine the train and select sets (keeping them as DataFrames/Series)\n",
    "X_train_select = \n",
    "y_train_select = \n",
    "\n",
    "# Fit the final Ridge model on the combined train+select data. X_train_select already includes an intercept column\n",
    "ridge_final = Ridge()\n",
    "ridge_final.fit()\n",
    "g_final =   # final model object\n",
    "\n",
    "# Display the final model coefficients with their feature names.\n",
    "coef_series = pd.Series()\n",
    "print(\"Final model coefficients:\")\n",
    "print()\n",
    "\n",
    "# Evaluate on the test set (which is also a DataFrame)\n",
    "preds_test = g_final.predict()\n",
    "test_oosRMSE = \n",
    "print(\"Test-set RMSE:\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bba0d8d-2481-4284-b9f0-2ae69d87c9a4",
   "metadata": {},
   "source": [
    "## Model Selection with Three Splits: Forward stepwise modeling\n",
    "\n",
    "We will use the adult data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2aa4eb-471b-46d8-a12a-782c07d2d9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the data from the CSV file\n",
    "adult = pd.read_csv(\"adult_data.csv\")\n",
    "\n",
    "#Remove observations with any missing values (similar to na.omit in R)\n",
    "adult = adult.dropna()\n",
    "\n",
    "#Check the number of observations\n",
    "n = adult.shape[0]\n",
    "\n",
    "print(\"Number of observations after dropping missing values:\", n)\n",
    "\n",
    "#Remove the \"education\" column (which is duplicative with education-num)\n",
    "if \"education\" in adult.columns:\n",
    "    adult = adult.drop(columns=[\"education\"])\n",
    "else:\n",
    "    print(\"Column 'education' not found; please check column names.\")\n",
    "\n",
    "p = adult.shape[1]\n",
    "print(\"Number of features after dropping education:\", p)\n",
    "\n",
    "#Inspect the first few rows\n",
    "print(adult.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb6638c-ba94-45d6-8eb7-9f8f04d83eb9",
   "metadata": {},
   "source": [
    "To implement forward stepwise, we need a \"full model\" that contains anything and everything we can possible want to use as transformed predictors. Let's first create log features of all the numeric features. Instead of pure log, use log(value + epsilon) to handle possible zeroes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fee285d-25a8-4a12-b77b-a623947d87a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspect the numeric features using describe()\n",
    "print(adult.describe())\n",
    "\n",
    "#Create log-transformed features (using log(value + epsilon) to avoid issues with zero values)\n",
    "epsilon = 1e-6\n",
    "adult['log_age'] = np.log(adult['age'] + epsilon)\n",
    "adult['log_fnlwgt'] = np.log(adult['fnlwgt'] + epsilon)\n",
    "adult['log_education_num'] = np.log(adult['education_num'] + epsilon)\n",
    "adult['log_capital_gain'] = np.log(adult['capital_gain'] + epsilon)\n",
    "adult['log_capital_loss'] = np.log(adult['capital_loss'] + epsilon)\n",
    "adult['log_hours_per_week'] = np.log(adult['hours_per_week'] + epsilon)\n",
    "\n",
    "#Inspect the first few rows to confirm the new columns\n",
    "print(adult.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84295804-3813-4095-ac40-942155b3b717",
   "metadata": {},
   "source": [
    "Now let's create a model matrix Xfull that contains all first order interactions. How many degrees of freedom in this \"full model\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1da2567-9001-40e3-8222-8bcbc92dd47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "#Load the adult data from the CSV file (exported from R)\n",
    "adult = pd.read_csv(\"adult_data.csv\")\n",
    "\n",
    "#Identify numeric and categorical columns.\n",
    "numeric_cols = adult.select_dtypes(include=['number']).columns.tolist()\n",
    "categorical_cols = adult.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(\"Numeric columns:\", )\n",
    "print(\"Categorical columns:\", )\n",
    "\n",
    "#Convert categorical columns to dummy variables using drop_first=True (producing k-1 columns per factor).\n",
    "adult_numeric = adult[]\n",
    "adult_categorical = \n",
    "adult_cat_dummies = pd.get_dummies(adult_categorical, drop_first=True)\n",
    "\n",
    "#Combine numeric and dummy columns\n",
    "adult_full = pd.concat()\n",
    "\n",
    "# Now create the full model matrix with main effects and all two-way interactions.\n",
    "pf = PolynomialFeatures()\n",
    "Xfull_array = pf.fit_transform()\n",
    "\n",
    "#Retrieve feature names\n",
    "try:\n",
    "    feature_names = pf.get_feature_names_out(adult_full.columns)\n",
    "except AttributeError:\n",
    "    feature_names = pf.get_feature_names(adult_full.columns)\n",
    "\n",
    "#Convert to a DataFrame\n",
    "Xfull = pd.DataFrame(Xfull_array, columns=feature_names)\n",
    "\n",
    "#Report the dimensions and degrees of freedom (number of columns)\n",
    "print(\"Dimensions of Xfull:\", )\n",
    "print(\"Degrees of freedom in the full model (including intercept):\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcefc1b-1519-42c7-8606-fe7cd5948782",
   "metadata": {},
   "source": [
    "Now let's split it into train, select and test sets. Because this will be a glm, model-building (training) will be slow, so let's keep the training set small at 2,000. Since prediction is fast, we can divide the others evenly among select and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0c8422-46e3-4270-94a5-4ef991fce4a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cfc22b2-4e6a-4738-84e0-f0bed99a7123",
   "metadata": {},
   "source": [
    "Now let's use the code from class to run the forward stepwise modeling. As this is binary classification, let's use logistic regression and to measure model performance, let's use the Brier score. Compute the Brier score in-sample (on training set) and oos (on selection set) for every iteration of j, the number of features selected from the greedy selection procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a589078b-91fd-4e3a-8b3e-db3a31c07973",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e632e55-a649-47c2-a826-3e0564748979",
   "metadata": {},
   "source": [
    "Plot the in-sample Brier score (in red) and oos Brier score (in blue) by the number of features used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd46da44-72f1-4bdf-a32e-f66148badfb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efae726b-efa1-4c4a-acc9-06468158ab69",
   "metadata": {},
   "source": [
    "Select the model with the best oos Brier score on the select set and find its oos Brier score on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be073e4-5a90-46cd-8b56-eba1baa6612a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83aeeaa4-c4b4-4694-b597-bc9eab33c3af",
   "metadata": {},
   "source": [
    "Create the final model object `g_final`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5056b290-a785-4f15-8d21-d09078695689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b34f6828-5974-429a-9021-3a4e9579d2a1",
   "metadata": {},
   "source": [
    "# Data Wrangling / Munging / Carpentry\n",
    "\n",
    "We will be using Pandas for the rest of this section for Data Wrangling/Munging/Carpentry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee4c497-f996-4062-8b1a-3164ec5dbb34",
   "metadata": {},
   "source": [
    "Load the `storms` dataset from the `dplyr` package and read about it using `?storms` and summarize its data via `skimr:skim`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6229f8f8-8252-455c-a615-6231a26627db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install skimpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa8fe54-1757-47ec-be8b-0635fd149805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from skimpy import skim\n",
    "\n",
    "#Load storms directly from dplyr’s GitHub (raw CSV) ———\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/\"\n",
    "    \"tidyverse/dplyr/master/data-raw/storms.csv\"\n",
    ")\n",
    "storms = pd.read_csv(url)  \n",
    "\n",
    "#Summarize via skimpy (like skimr::skim) ———\n",
    "skim(storms)\n",
    "\n",
    "#Show the first few rows (like head()) ———\n",
    "print(storms.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f1e843-2677-4660-90da-989456faf87f",
   "metadata": {},
   "source": [
    "To make the modeling exercise easier, let's eliminate rows that have missingness in `tropicalstorm_force_diameter` or `hurricane_force_diameter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0b2d09-7e0a-468e-8afb-f08ad2fdd957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NA in either diameter column\n",
    "storms_clean = storms.dropna(\n",
    "    subset=[\"tropicalstorm_force_diameter\", \"hurricane_force_diameter\"]\n",
    ")\n",
    "\n",
    "# Reset the index so skimpy’s argmin+loc hack works\n",
    "storms_clean = storms_clean.reset_index(drop=True)\n",
    "\n",
    "# See how many rows you kept\n",
    "print(f\"Kept {len(storms_clean)} of {len(storms)} rows\")\n",
    "\n",
    "#Summarize the cleaned data\n",
    "skim(storms_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a955cb-fb5e-4a99-bbdc-e7c1fa4d5e6a",
   "metadata": {},
   "source": [
    "Which column(s) should be converted to type factor? Do the conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36abcb53-4654-4b9e-9246-9ea2729fefb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to categorical (factor) dtype\n",
    "storms_clean['status'] = storms_clean['status'].astype('category')\n",
    "storms_clean['category'] = storms_clean['category'].astype('category')\n",
    "\n",
    "# Verify\n",
    "print(storms_clean.dtypes[['status', 'category']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fcb8ff-b95a-48cd-9399-517ef8af9232",
   "metadata": {},
   "source": [
    "Reorder the columns so name is first, status is second, category is third and the rest are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fb1aa8-c5a7-4e1b-8944-735010c6a907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns so that 'name', 'status', 'category' come first by using a list comprehension\n",
    "cols = \n",
    "new_order = \n",
    "\n",
    "# Apply the new column order\n",
    "storms_clean = storms_clean[new_order]\n",
    "\n",
    "# (Optional) Verify the new order\n",
    "print(storms_clean.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be871e69-17d9-4a81-b6dc-a727b2a1c7ab",
   "metadata": {},
   "source": [
    "Find a subset of the data of storms only in the 1970's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea1c85e-633f-4d35-b083-3cbd2082baa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset to storms in the 1970s (i.e. years 1970–1979 inclusive)\n",
    "storms_1970s = storms_clean.copy()\n",
    "storms_1970s = \n",
    "\n",
    "#Check how many rows and peek at the first few\n",
    "print(f\"Number of 1970s storm records: {}\")\n",
    "print(storms_1970s.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6af50d-3beb-4c5b-83a9-e650605186d5",
   "metadata": {},
   "source": [
    "Find a subset of the data of storm observations only with category 4 and above and wind speed 100MPH and above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6541be-7f13-4a76-84db-761a7dba411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset to Saffir–Simpson category 4 or 5 AND wind ≥ 100 mph\n",
    "storms_cat4plus = storms_clean.copy()\n",
    "\n",
    "#Ensure 'category' is numeric (coerce non‑numbers to NaN)\n",
    "storms_cat4plus['category'] = pd.to_numeric()\n",
    "\n",
    "#Build your mask with explicit parentheses\n",
    "mask = (\n",
    "           # drop missing categories\n",
    "           # keep only cat 4 or 5\n",
    "           # wind at least 100 mph\n",
    ")\n",
    "\n",
    "#Apply to the same DataFrame and reset the index\n",
    "storms_cat4plus = storms_cat4plus.\n",
    "\n",
    "#Sanity check\n",
    "print(f\"Found {len(storms_cat4plus)} records:\")\n",
    "print(storms_cat4plus[['name','status','category','wind']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2716cd-5c44-47c1-9152-a34c8bf8c771",
   "metadata": {},
   "source": [
    "Create a new feature `wind_speed_per_unit_pressure`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e3f0ca-b187-41e7-b10c-3a42489de307",
   "metadata": {},
   "outputs": [],
   "source": [
    "storms_clean = storms_clean.copy()\n",
    "storms_clean['wind_speed_per_unit_pressure'] = (\n",
    "    \n",
    ")\n",
    "\n",
    "# Verify it was added:\n",
    "print(storms_clean[['wind','pressure','wind_speed_per_unit_pressure']].head())\n",
    "storms_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933494f7-bf4f-46de-8df6-d8c8c029d2a9",
   "metadata": {},
   "source": [
    "Create a new feature: `average_diameter` which averages the two diameter metrics. If one is missing, then use the value of the one that is present. If both are missing, leave missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba567ad-cb46-48b0-b2c6-26165802412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "storms_clean['average_diameter'] = storms_clean[\n",
    "    []\n",
    "].mean()\n",
    "\n",
    "#Verify it worked:\n",
    "print(storms_clean[[]].head())\n",
    "storms_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfe5eeb-9d31-4381-b3bf-54b5857ef810",
   "metadata": {},
   "source": [
    "For each storm, summarize the maximum wind speed. \"Summarize\" means create a new dataframe with only the summary metrics you care about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162e4281-922d-4dbf-a90c-ae90c8304d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group by storm name and compute the maximum wind speed\n",
    "max_wind_per_storm = (\n",
    "    storms.groupby()\n",
    ")\n",
    "\n",
    "#Inspect the result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f76338-556d-45dc-95fa-f226cf5c25d2",
   "metadata": {},
   "source": [
    "Order your dataset by maximum wind speed storm but within the rows of storm show the observations in time order from early to late."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493e98ab-234d-421a-b476-8c6a2a00ba0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a datetime column from year, month, day, hour\n",
    "storms['datetime'] = pd.to_datetime(\n",
    "    \n",
    ")\n",
    "\n",
    "#Compute each storm’s maximum wind speed\n",
    "storms['max_wind'] = \n",
    "\n",
    "#Sort storms by max_wind descending, and within each storm by datetime ascending\n",
    "storms_sorted = storms.sort_values(\n",
    "  \n",
    ").reset_index(drop=True)\n",
    "\n",
    "storms_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a09a1b-527b-4204-86e6-160e918ec3f2",
   "metadata": {},
   "source": [
    "Find the strongest storm by wind speed per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478abe7a-218c-40e9-8074-42244b5e4714",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute each storm’s maximum wind speed per year\n",
    "storm_max = (\n",
    "    storms\n",
    "    .groupby(['year', 'name'], as_index=False)\n",
    "    .agg(max_wind=('wind', 'max'))\n",
    ")\n",
    "\n",
    "#For each year, select the storm with the highest max_wind\n",
    "idx = \n",
    "strongest_per_year = (\n",
    "    \n",
    ")\n",
    "\n",
    "#Inspect the result\n",
    "print(strongest_per_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea509bd-d3b9-4f81-b330-6b033f300472",
   "metadata": {},
   "source": [
    "For each named storm, find its maximum category, wind speed, pressure and diameters. Do not allow the max to be NA (unless all the measurements for that storm were NA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc358951-c159-4a38-8ad9-be85c3129773",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure 'category' is a nullable integer so max skips NA by default\n",
    "storms['category'] = pd.to_numeric(storms['category'], errors='coerce').astype('Int64')\n",
    "\n",
    "#Group by storm name and compute the maxima (skipna=True by default)\n",
    "summary = storms.groupby('name', as_index=False).agg(\n",
    "    max_category=(),\n",
    "    max_wind=(),\n",
    "    max_pressure=(),\n",
    "    max_tropicalstorm_diameter=(),\n",
    "    max_hurricane_diameter=()\n",
    ")\n",
    "\n",
    "#Inspect the first few rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1287c97f-d64d-4cb9-a4dd-de7985d042c1",
   "metadata": {},
   "source": [
    "For each year in the dataset, tally the number of storms. \"Tally\" is a fancy word for \"count the number of\". Plot the number of storms by year. Any pattern?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a01653-c6f4-42a1-8768-3433755e9643",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import ggplot, aes, geom_line, labs\n",
    "\n",
    "\n",
    "#Tally the number of unique named storms per year\n",
    "storms_per_year = storms.groupby()\n",
    "\n",
    "\n",
    "#Plot with plotnine\n",
    "plot = (ggplot(storms_per_year, aes(x='year', y='count'))+\n",
    "        geom_line() +\n",
    "        geom_point() + \n",
    "        labs(x='Year', y='Number of Named Storms', title='Named Storms per Year')\n",
    "        )\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3220da96-097c-4889-b853-8d75b0f34736",
   "metadata": {},
   "source": [
    "For each year in the dataset, tally the storms by category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d876d1-c353-4460-8b26-70fa865f0ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensure 'category' is numeric (so missing stays as NaN)\n",
    "storms['category'] = \n",
    "\n",
    "#Drop missing categories (if you only want actual hurricane categories)\n",
    "storms_cat = s\n",
    "\n",
    "#Tally unique storms by year and category\n",
    "storms_by_category = (\n",
    "    \n",
    ")\n",
    "\n",
    "#Pivot to get categories as columns\n",
    "storms_by_category_wide = storms_by_category.pivot()\n",
    "    \n",
    "\n",
    "#Inspect the long‐format tally:\n",
    "print(storms_by_category)\n",
    "\n",
    "#Inspect the wide‐format tally:\n",
    "print(storms_by_category_wide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584e96d1-d8d5-4f7f-a589-eff15f2cc43c",
   "metadata": {},
   "source": [
    "For each year in the dataset, find the maximum wind speed per status level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fef352-75ce-4b20-b16c-8df2f80c8a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensure 'status' is treated as a category\n",
    "storms['status'] = \n",
    "\n",
    "#Group by year and status, then compute the max wind speed in each group\n",
    "max_wind_by_year_status = (\n",
    "    \n",
    ")\n",
    "\n",
    "#Note: Groupby on a categorical will produce all combinations of year and every possible status ... even those that never actually occur, \n",
    "# to prevent this we set observed=True\n",
    "\n",
    "\n",
    "#Inspect the result\n",
    "print(max_wind_by_year_status.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7312bd56-656d-4006-859c-a305a05e84a1",
   "metadata": {},
   "source": [
    "For each storm, summarize its average location in latitude / longitude coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ba1e31-5a9d-4297-96b5-d14287d95575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by storm name and compute average latitude & longitude\n",
    "avg_location = (\n",
    " \n",
    ")\n",
    "\n",
    "# Inspect the result\n",
    "print(avg_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb3d9bf-6413-42ec-8341-b8f2e23b1322",
   "metadata": {},
   "source": [
    "For each storm, summarize its duration in number of hours (to the nearest 6hr increment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ced6c5-4c2a-4edd-8784-99d131093d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a datetime column from year, month, day, hour\n",
    "storms['datetime'] = pd.to_datetime(\n",
    "\n",
    ")\n",
    "\n",
    "#For each storm, find the earliest and latest observation\n",
    "duration = (\n",
    "  \n",
    ")\n",
    "\n",
    "#Compute duration in hours\n",
    "duration['duration_hours'] = (\n",
    "   \n",
    ")\n",
    "\n",
    "#Round to the nearest 6‑hour increment\n",
    "duration['duration_6hr_increment'] = \n",
    "\n",
    "# Select the summary columns\n",
    "storm_durations = \n",
    "\n",
    "#Inspect the result\n",
    "print(storm_durations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503a0fa7-11ae-40a3-8a6b-0da25c899a34",
   "metadata": {},
   "source": [
    "For storm in a category, create a variable `storm_number` that enumerates the storms 1, 2, ... (in date order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3685ee9-4f30-46a4-854c-68ae02483184",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the storms data\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/\"\n",
    "    \"tidyverse/dplyr/master/data-raw/storms.csv\"\n",
    ")\n",
    "storms = pd.read_csv(url)\n",
    "\n",
    "#Coerce category to numeric (so blanks → NaN) and drop storms with no category\n",
    "storms['category'] = pd.to_numeric(storms['category'], errors='coerce').astype('Int64')\n",
    "\n",
    "#Build a datetime column for ordering\n",
    "storms['datetime'] = pd.to_datetime(\n",
    "    dict(\n",
    "        year=storms['year'],\n",
    "        month=storms['month'],\n",
    "        day=storms['day'],\n",
    "        hour=storms['hour']\n",
    "    )\n",
    ")\n",
    "\n",
    "#Summarize each storm’s start time and its (max) category\n",
    "storm_meta = (\n",
    "    storms\n",
    "    .groupby('name', as_index=False)\n",
    "    .agg(\n",
    "        start_time    = ('datetime', 'min'),\n",
    "        storm_category= ('category','max')\n",
    "    )\n",
    "    # drop any storms that never reached a hurricane category\n",
    "    .dropna(subset=['storm_category'])\n",
    ")\n",
    "storm_meta['storm_category'] = storm_meta['storm_category'].astype(int)\n",
    "\n",
    "#Sort by category, then by start_time\n",
    "storm_meta = storm_meta.sort_values(['storm_category','start_time'])\n",
    "\n",
    "#Within each category, enumerate storms in date order\n",
    "storm_meta['storm_number'] = storm_meta.groupby('storm_category').cumcount() + 1\n",
    "\n",
    "#Merge the storm_number back onto every observation\n",
    "storms = storms.merge(\n",
    "    storm_meta[['name','storm_number']],\n",
    "    on='name',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Now `storms` has a `storm_number` column that runs 1,2,3… within each category\n",
    "# Verify:\n",
    "print(storm_meta.head(10))\n",
    "print(storms[['name','category','datetime','storm_number']].drop_duplicates())\n",
    "print(storms[['name','category','datetime','storm_number']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc9c7da-7c69-46e8-a305-c41a6905ddd0",
   "metadata": {},
   "source": [
    "Convert year, month, day, hour into the variable `timestamp` using the `lubridate` package. Although the new package `clock` just came out, `lubridate` still seems to be standard. Next year I'll probably switch the class to be using `clock`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cfac8d-a601-482d-8ba7-63b2e85193bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use pandas' to_datetime on a dict of the separate columns\n",
    "storms['timestamp'] = pd.to_datetime({\n",
    "\n",
    "})\n",
    "\n",
    "# Verify\n",
    "print(storms[['year','month','day','hour','timestamp']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719023d4-fe90-47f6-bbe1-7f5e9f3e9eea",
   "metadata": {},
   "source": [
    "Create new variables `day_of_week` which is a factor with levels \"Sunday\", \"Monday\", ... \"Saturday\" and `week_of_year` which is integer 1, 2, ..., 52."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d520b495-fc51-4ade-a9ef-9c8a7d6494a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create day_of_week as an ordered categorical (Sunday → Saturday)\n",
    "dow_order = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
    "storms['day_of_week'] = pd.Categorical(\n",
    ")\n",
    "\n",
    "# Create week_of_year as the ISO week number (1–52/53)\n",
    "storms['week_of_year'] = \n",
    "\n",
    "#Verify\n",
    "print(storms[['timestamp','day_of_week','week_of_year']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba161b03-d05d-471f-8ee2-9e35cdad793b",
   "metadata": {},
   "source": [
    "For each storm, summarize the day in which is started in the following format \"Friday, June 27, 1975\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aee056-e65c-4f56-a829-faf16a2797ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each storm, find its start time (earliest timestamp)\n",
    "start_times = (\n",
    "\n",
    ")\n",
    "\n",
    "#Format that start_time into \"Friday, June 27, 1975\"\n",
    "start_times['start_day'] = (\n",
    "    start_times['start_time'].dt.day_name() + \", \"\n",
    "    + start_times['start_time'].dt.month_name() + \" \"\n",
    "    + start_times['start_time'].dt.day.astype(str) + \", \"\n",
    "    + start_times['start_time'].dt.year.astype(str)\n",
    ")\n",
    "\n",
    "#Inspect the result\n",
    "print(start_times[['name', 'start_day']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42601271-2b69-424a-b780-f4b2909e96a1",
   "metadata": {},
   "source": [
    "Create a new factor variable `decile_windspeed` by binning wind speed into 10 bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f40799-e19a-40c3-a6cb-33dd8f1b6412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create decile labels 1–10, pd.qcut will bin into 10 quantile‐based groups\n",
    "storms['decile_windspeed'] = pd.qcut(\n",
    "   \n",
    ")\n",
    "\n",
    "#Convert to a categorical dtype (ordered)\n",
    "storms['decile_windspeed'] = \n",
    "\n",
    "#Verify\n",
    "print(storms[['wind', 'decile_windspeed']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8faa5c-eab0-4905-89e9-65445f66bbb4",
   "metadata": {},
   "source": [
    "Create a new data frame `serious_storms` which are category 3 and above hurricanes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaa7807-4634-46a6-a5be-c1e14d3bc47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coerce 'category' to numeric so that non‑hurricanes become NaN\n",
    "storms['category'] = \n",
    "\n",
    "#Filter to only category 3, 4, or 5 storms\n",
    "serious_storms = \n",
    "\n",
    "#Inspect\n",
    "print(f\"Found {} category 3+ storms\")\n",
    "print(serious_storms[['name','status','category','wind']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b35aa63-43f9-4353-9c50-ed8ece17aeff",
   "metadata": {},
   "source": [
    "In `serious_storms`, merge the variables lat and long together into `lat_long` with values `lat / long` as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb521f89-8484-4468-a3aa-1fe08be6d50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Merge lat & long into a single string column \"lat_long\"\n",
    "serious_storms['lat_long'] = (\n",
    " \n",
    ")\n",
    "\n",
    "# 2. Inspect the new column\n",
    "print(serious_storms[['lat', 'long', 'lat_long']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498023d2-3106-48b5-a275-fd804edede03",
   "metadata": {},
   "source": [
    "Let's return now to the original storms data frame. For each category, find the average wind speed, pressure and diameters (do not count the NA's in your averaging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241e6e7f-d942-4b34-9c9b-2b694feaad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensure 'category' is numeric (so non‑hurricanes become NaN)\n",
    "storms['category'] = pd.to_numeric(storms['category'], errors='coerce')\n",
    "\n",
    "#Drop rows with missing category (so we only average actual Saffir–Simpson categories)\n",
    "storms_cat = storms.dropna()\n",
    "\n",
    "#Group by category and compute means (skipna=True by default)\n",
    "category_summary = (\n",
    "  \n",
    ")\n",
    "\n",
    "# 5. View the result\n",
    "print(category_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a047ca-db58-49bf-a00d-7b7bf16fddca",
   "metadata": {},
   "source": [
    "For each named storm, find its maximum category, wind speed, pressure and diameters (do not allow the max to be NA) and the number of readings (i.e. observations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f85adb0-b9d9-4e74-944b-5f0de78c1b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load the raw storms data\n",
    "url = \"https://raw.githubusercontent.com/tidyverse/dplyr/master/data-raw/storms.csv\"\n",
    "storms = pd.read_csv(url)\n",
    "\n",
    "# 2. Coerce category to numeric (bad/missing → NaN)\n",
    "storms['category'] = pd.to_numeric(storms['category'], errors='coerce')\n",
    "\n",
    "# 3. Drop any rows where category, wind, pressure or either diameter is missing\n",
    "cols_to_keep = [\n",
    "    'category',\n",
    "    'wind',\n",
    "    'pressure',\n",
    "    'tropicalstorm_force_diameter',\n",
    "    'hurricane_force_diameter'\n",
    "]\n",
    "storms_clean = storms.dropna(subset=cols_to_keep)\n",
    "\n",
    "# 4. Now summarize per storm\n",
    "summary = storms_clean.groupby('name', as_index=False).agg(\n",
    "    max_category                 = ('category',                   'max'),\n",
    "    max_wind                     = ('wind',                       'max'),\n",
    "    max_pressure                 = ('pressure',                   'max'),\n",
    "    max_tropicalstorm_diameter   = ('tropicalstorm_force_diameter','max'),\n",
    "    max_hurricane_diameter       = ('hurricane_force_diameter',   'max'),\n",
    "    n_readings                   = ('name',                       'size')\n",
    ")\n",
    "\n",
    "# 5. If you want max_category as an integer:\n",
    "summary['max_category'] = summary['max_category'].astype(int)\n",
    "\n",
    "print(summary.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd0e134-917f-42b4-a651-32285a95bacb",
   "metadata": {},
   "source": [
    "Calculate the distance from each storm observation to Miami in a new variable `distance_to_miami`. This is very challenging. You will need a function that computes distances from two sets of latitude / longitude coordinates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7d87e7-2e9e-4a93-88a5-4b57e737a426",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a vectorized Haversine function (returns distance in kilometers)\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the great‐circle distance between two points \n",
    "    on the Earth (specified in decimal degrees) using the Haversine formula.\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    φ1, φ2 = np.radians(lat1), np.radians(lat2)\n",
    "    Δφ    = np.radians(lat2 - lat1)\n",
    "    Δλ    = np.radians(lon2 - lon1)\n",
    "\n",
    "    a = np.sin(Δφ/2)**2 + np.cos(φ1) * np.cos(φ2) * np.sin(Δλ/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    R = 6371  # Earth radius in kilometers\n",
    "    return R * c\n",
    "\n",
    "# 3. Miami’s coordinates\n",
    "miami_lat, miami_lon = 25.7617, -80.1918\n",
    "\n",
    "# 4. Compute distance for every observation\n",
    "storms['distance_to_miami_km'] = haversine(\n",
    "\n",
    ")\n",
    "\n",
    "#also in miles\n",
    "storms['distance_to_miami_miles'] = storms['distance_to_miami_km'] * 0.621371\n",
    "\n",
    "# 5. Verify\n",
    "print(storms[['lat','long','distance_to_miami_km','distance_to_miami_miles']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0731ce-dfc4-49ef-9f70-ecdbf0534a23",
   "metadata": {},
   "source": [
    "For each storm observation, use the function from the previous question to calculate the distance it moved since the previous observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218aca14-3aa8-4b8f-9f0d-3a9091e94fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a proper timestamp for ordering\n",
    "storms['timestamp'] = pd.to_datetime({\n",
    "    'year':  storms['year'],\n",
    "    'month': storms['month'],\n",
    "    'day':   storms['day'],\n",
    "    'hour':  storms['hour']\n",
    "})\n",
    "\n",
    "#Sort by storm name and time\n",
    "storms = storms.sort_values(['name', 'timestamp'])\n",
    "\n",
    "#Shift lat/long within each storm to get the previous observation\n",
    "storms['prev_lat']  = storms.groupby('name')['lat'].shift(1)\n",
    "storms['prev_long'] = storms.groupby('name')['long'].shift(1)\n",
    "\n",
    "#Compute distance moved since the previous observation\n",
    "storms['distance_moved_km'] = haversine(\n",
    "    \n",
    ")\n",
    "\n",
    "#Fill NaN for the first observation of each storm with 0\n",
    "storms['distance_moved_km'] = storms['distance_moved_km'].fillna(0)\n",
    "\n",
    "#Inspect the result\n",
    "print(storms[['name','timestamp','prev_lat','prev_long','distance_moved_km']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2527af0-4251-4277-a3a6-65a20acbf424",
   "metadata": {},
   "source": [
    "For each storm, find the total distance it moved over its observations and its total displacement. \"Distance\" is a scalar quantity that refers to \"how much ground an object has covered\" during its motion. \"Displacement\" is a vector quantity that refers to \"how far out of place an object is\"; it is the object's overall change in position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144ebdaa-a1d8-4bb4-a642-192b92a6e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute distance moved between consecutive observations\n",
    "storms['prev_lat']  = storms.groupby('name')['lat'].shift(1)\n",
    "storms['prev_long'] = storms.groupby('name')['long'].shift(1)\n",
    "storms['distance_moved_km'] = haversine(\n",
    "    storms['prev_lat'], storms['prev_long'],\n",
    "    storms['lat'],        storms['long']\n",
    ").fillna(0)\n",
    "\n",
    "#Summarize per storm: total distance and displacement\n",
    "summary = (\n",
    "    storms\n",
    "    .groupby('name', as_index=False)\n",
    "    .agg(\n",
    "        total_distance_km = ('distance_moved_km', 'sum'),\n",
    "        start_lat         = ('lat',               'first'),\n",
    "        start_long        = ('long',              'first'),\n",
    "        end_lat           = ('lat',               'last'),\n",
    "        end_long          = ('long',              'last')\n",
    "    )\n",
    ")\n",
    "\n",
    "#Compute displacement (straight‐line distance from start to end)\n",
    "summary['displacement_km'] = haversine(\n",
    "   \n",
    ")\n",
    "\n",
    "#Drop the start/end coords if you only want the metrics\n",
    "storm_movement = summary[['name', 'total_distance_km', 'displacement_km']]\n",
    "\n",
    "print(storm_movement.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9762059f-c0b2-4089-a111-4f799a86443e",
   "metadata": {},
   "source": [
    "For each storm observation, calculate the average speed the storm moved in location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952c2042-e1df-466f-b84a-c6efa0b59bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute distance moved between consecutive observations\n",
    "storms['prev_lat'] = storms.groupby('name')['lat'].shift(1)\n",
    "storms['prev_long'] = \n",
    "storms['distance_km'] = haversine(\n",
    "    storms['prev_lat'], storms['prev_long'],\n",
    "    storms['lat'], storms['long']\n",
    ").fillna(0)\n",
    "\n",
    "#Compute time difference (in hours) since previous observation\n",
    "storms['prev_time'] = storms.groupby('name')['timestamp'].shift(1)\n",
    "\n",
    "\n",
    "#Calculate average speed (km/h) for each observation\n",
    "storms['avg_speed_kmh'] = storms['distance_km'] / storms['time_diff_hr']\n",
    "\n",
    "#Handle the first observation of each storm (where time_diff_hr is 0)\n",
    "storms.loc[storms['time_diff_hr'] == 0, 'avg_speed_kmh'] = 0\n",
    "\n",
    "# Inspect the result\n",
    "print(storms[['name','timestamp','distance_km','time_diff_hr','avg_speed_kmh']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d246c8a1-58dd-4cb5-a407-7b04deddccd4",
   "metadata": {},
   "source": [
    "For each storm, calculate its average ground speed (how fast its eye is moving which is different from windspeed around the eye)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794dc7d6-cce5-4a32-9a74-172f3674f632",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group by storm name and sum distance and time\n",
    "storm_movement = (\n",
    "    \n",
    ")\n",
    "\n",
    "#Compute average ground speed (km/h)\n",
    "storm_movement['avg_ground_speed_kmh'] = (\n",
    ")\n",
    "\n",
    "#Handle storms with only one observation (total_time_hr = 0)\n",
    "storm_movement.loc[\n",
    "  \n",
    "#Inspect the result\n",
    "print(storm_movement[['name', 'avg_ground_speed_kmh']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5be41e-99f1-4642-bbe9-9385314c9b85",
   "metadata": {},
   "source": [
    "Is there a relationship between average ground speed and maximum category attained? Use a dataframe summary (not a regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ee75a4-d72f-4921-8087-dc249a1d3489",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute maximum category per storm ---\n",
    "storms['category'] = pd.to_numeric(storms['category'], errors='coerce').astype('Int64')\n",
    "max_cat = (\n",
    "    storms\n",
    "    .groupby('name', as_index=False)\n",
    "    .agg(max_category=('category', 'max'))\n",
    "    .fillna(0)\n",
    ")\n",
    "max_cat['max_category'] = max_cat['max_category'].astype(int)\n",
    "\n",
    "#Merge avg ground speed with max category ---\n",
    "df = (\n",
    "   \n",
    ")\n",
    "\n",
    "#Summarize relationship by max_category ---\n",
    "relationship_summary = (\n",
    "   \n",
    ")\n",
    "\n",
    "print(relationship_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2497e888-f57b-429c-a088-54a56784161c",
   "metadata": {},
   "source": [
    "Now we want to transition to building real design matrices for prediction. This is more in tune with what happens in the real world. Large data dump and you convert it into $X$ and $y$ how you see fit.\n",
    "\n",
    "Suppose we wish to predict the following: given the first three readings of a storm, can you predict its maximum wind speed? Identify the `y` and identify which features you need $x_1, ... x_p$ and build that matrix with `dplyr` functions. This is not easy, but it is what it's all about. Feel free to \"featurize\" as creatively as you would like. You aren't going to overfit if you only build a few features relative to the total 198 storms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43acb482-33c9-49ed-8e35-089480a37167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Load the raw storms data\n",
    "url = \"https://raw.githubusercontent.com/tidyverse/dplyr/master/data-raw/storms.csv\"\n",
    "storms = pd.read_csv(url)\n",
    "\n",
    "#Build a proper timestamp and coerce numeric columns\n",
    "storms['timestamp'] = pd.to_datetime({\n",
    "    'year':  storms['year'],\n",
    "    'month': storms['month'],\n",
    "    'day':   storms['day'],\n",
    "    'hour':  storms['hour']\n",
    "})\n",
    "storms['category'] = pd.to_numeric(storms['category'], errors='coerce')\n",
    "\n",
    "#Drop any observation missing category or either diameter\n",
    "storms_clean = storms.dropna(subset=[\n",
    "    'category',\n",
    "    'tropicalstorm_force_diameter',\n",
    "    'hurricane_force_diameter'\n",
    "]).copy()\n",
    "\n",
    "#Sort by storm name & time\n",
    "storms_clean = storms_clean.sort_values(['name','timestamp'])\n",
    "\n",
    "#Keep only storms with at least 3 readings\n",
    "counts = storms_clean.groupby('name').size()\n",
    "valid_storms = counts[counts >= 3].index\n",
    "storms_clean = storms_clean[storms_clean['name'].isin(valid_storms)]\n",
    "\n",
    "#Define the target y: maximum wind per storm\n",
    "y = (\n",
    "    storms_clean\n",
    "    .groupby('name')['wind']\n",
    "    .max()\n",
    "    .rename('max_wind')\n",
    ")\n",
    "\n",
    "#Feature engineering on the *first three* readings of each storm\n",
    "first3 = (\n",
    "    storms_clean\n",
    "    .groupby('name')\n",
    "    .head(3)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "#Aggregate features from those first 3 readings\n",
    "features = (\n",
    "    first3\n",
    "    .groupby('name')\n",
    "    .agg(\n",
    "        wind_mean3            = ('wind',                         'mean'),\n",
    "        pressure_mean3        = ('pressure',                     'mean'),\n",
    "        ts_diameter_mean3     = ('tropicalstorm_force_diameter', 'mean'),\n",
    "        hurr_diameter_mean3   = ('hurricane_force_diameter',    'mean'),\n",
    "        category_first        = ('category',                     'first'),\n",
    "        time_of_day_first_hr  = ('timestamp', \n",
    "                                 lambda x: x.dt.hour.iloc[0])\n",
    "    )\n",
    ")\n",
    "\n",
    "#Merge X and y into one DataFrame\n",
    "data = features.merge(y, left_index=True, right_index=True)\n",
    "\n",
    "#Separate X and y\n",
    "X = data.drop(columns='max_wind')\n",
    "y = data['max_wind']\n",
    "\n",
    "#Reset index\n",
    "X = X.reset_index(drop=True)\n",
    "y = y.reset_index(drop=True)\n",
    "\n",
    "#Inspect shapes\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"\\nFirst few rows of X:\")\n",
    "print(X.head())\n",
    "print(\"\\nFirst few values of y:\")\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f134581d-f3f2-455f-8566-7aba6a1ca2e7",
   "metadata": {},
   "source": [
    "Fit your model. Validate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27e0dd6-aea2-4cda-9bf8-3f8fa8bbb82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "#Split into train & test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split()\n",
    "\n",
    "#Instantiate the model\n",
    "model = RandomForestRegressor(\n",
    "\n",
    ")\n",
    "\n",
    "#Fit on the training data\n",
    "model.fit()\n",
    "\n",
    "#Predict on the test set\n",
    "y_pred = model.predict()\n",
    "\n",
    "#Evaluate performance\n",
    "mse   = mean_squared_error(y_test, y_pred)\n",
    "r2    = r2_score(y_test, y_pred)\n",
    "print(f\"Test MSE:  {mse:.2f}\")\n",
    "print(f\"Test R²:   {r2:.2f}\")\n",
    "\n",
    "#Cross‑validate (5‑fold) on the full dataset. We'll look at both MSE and R²\n",
    "cv_mse_scores = -cross_val_score(\n",
    "    \n",
    ")\n",
    "cv_r2_scores  = cross_val_score(\n",
    "    \n",
    ")\n",
    "\n",
    "print(\"\\n5‑Fold CV results:\")\n",
    "print(f\"Mean CV MSE:  {cv_mse_scores.mean():.2f} ± {cv_mse_scores.std():.2f}\")\n",
    "print(f\"Mean CV R²:   {cv_r2_scores.mean():.2f} ± {cv_r2_scores.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a87e4fe-e067-4ca5-9c68-d6ebef8cb50d",
   "metadata": {},
   "source": [
    "Assess your level of success at this endeavor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50a1e10-6ecd-452b-844e-294c242ab729",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76e030ab-357f-4ed6-9fd3-737670d095b0",
   "metadata": {},
   "source": [
    "# More data munging with table joins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de3ea0f-82b8-41c5-8362-008c89a84985",
   "metadata": {},
   "source": [
    "We will be using the `storms` dataset from the `dplyr` package. Filter this dataset on all storms that have no missing measurements for the two diameter variables, \"tropicalstorm_force_diameter\" and \"hurricane_force_diameter\". Zeroes count as missing as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54864ba1-f1ca-4207-8391-48e7e87ab3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Load the storms data from dplyr’s GitHub\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/\"\n",
    "    \"tidyverse/dplyr/master/data-raw/storms.csv\"\n",
    ")\n",
    "storms = pd.read_csv(url)\n",
    "\n",
    "#Filter out rows where either diameter is missing or zero\n",
    "filtered = storms.loc[\n",
    "    storms['tropicalstorm_force_diameter'].notna()\n",
    "    & (storms['tropicalstorm_force_diameter'] != 0)\n",
    "    & storms['hurricane_force_diameter'].notna()\n",
    "    & (storms['hurricane_force_diameter'] != 0)\n",
    "].reset_index(drop=True)\n",
    "\n",
    "#Inspect the result\n",
    "print(f\"Kept {len(filtered)} rows out of {len(storms)}\")\n",
    "print(filtered[['tropicalstorm_force_diameter','hurricane_force_diameter']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af8ae9c-5b5b-41b8-987f-eaa1e3d74bdd",
   "metadata": {},
   "source": [
    "From this subset, create a data frame that only has storm name, observation period number for each storm (i.e., 1, 2, ..., T) and the \"tropicalstorm_force_diameter\" and \"hurricane_force_diameter\" metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb355857-c610-4168-b3b6-036400605fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a timestamp so we can order observations chronologically\n",
    "filtered['timestamp'] = pd.to_datetime({\n",
    "    'year':  filtered['year'],\n",
    "    'month': filtered['month'],\n",
    "    'day':   filtered['day'],\n",
    "    'hour':  filtered['hour']\n",
    "})\n",
    "\n",
    "#Sort by storm name and time\n",
    "filtered = \n",
    "\n",
    "#Assign an observation period number within each storm\n",
    "filtered['period'] = \n",
    "\n",
    "#Select only the desired columns\n",
    "result = filtered[[\n",
    "    'name',\n",
    "    'period',\n",
    "    'tropicalstorm_force_diameter',\n",
    "    'hurricane_force_diameter'\n",
    "]]\n",
    "\n",
    "#Inspect\n",
    "print(result.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c3600d-0612-463a-85a7-140697d4f6c1",
   "metadata": {},
   "source": [
    "Create a data frame in long format with columns \"diameter\" for the measurement and \"diameter_type\" which will be categorical taking on the values \"hu\" or \"ts\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53a1a0d-a7b6-49a3-a324-61c68f3ce5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Melt into long format\n",
    "long_df = result.melt(\n",
    "    \n",
    ")\n",
    "\n",
    "# Map the column names to 'ts' and 'hu' and make it categorical\n",
    "long_df['diameter_type'] = long_df['diameter_type'].map({\n",
    "    'tropicalstorm_force_diameter': 'ts',\n",
    "    'hurricane_force_diameter':     'hu'\n",
    "}).astype(pd.CategoricalDtype(categories=['ts', 'hu']))\n",
    "\n",
    "# 4. Inspect\n",
    "print(long_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93656f54-772d-480b-8fe9-e7d94695ad8a",
   "metadata": {},
   "source": [
    "Using this long-formatted data frame, use a line plot to illustrate both \"tropicalstorm_force_diameter\" and \"hurricane_force_diameter\" metrics by observation period for four random storms using a 2x2 faceting. The two diameters should appear in two different colors and there should be an appropriate legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24e5c0b-73b9-4aa0-b1c8-8c729a3a3c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import ggplot, aes, geom_line, facet_wrap, labs, theme_minimal\n",
    "\n",
    "#Pick four random storms\n",
    "np.random.seed(42)\n",
    "storm_names = long_df['name'].unique()\n",
    "selected = \n",
    "\n",
    "#Subset to those four storms\n",
    "plot_df = long_df[long_df['name'].isin()].copy()\n",
    "\n",
    "#Build the line‐plot with 2×2 faceting\n",
    "plot = (\n",
    "    ggplot(plot_df, aes(x='period', y='diameter',\n",
    "                        color='diameter_type',\n",
    "                        group='diameter_type')) + \n",
    "    geom_line() +\n",
    "    facet_wrap('~name', ncol=2) +\n",
    "    labs(\n",
    "        x='Observation Period',\n",
    "        y='Diameter (nautical miles)',\n",
    "        color='Diameter Type',\n",
    "        title='Tropical Storm vs Hurricane Force Diameters\\nOver Observation Period for Four Random Storms'\n",
    "    ) +\n",
    "    theme_minimal()\n",
    ")\n",
    "\n",
    "plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
