{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b5c02f3-9161-410f-a522-d6aea83a5ae6",
   "metadata": {},
   "source": [
    "## Course Assignment Instructions\n",
    "You should have Python (version 3.8 or later) and Jupyter Notebook installed to complete this assignment. You will write code in the empty cell/cells below the problem. While most of this will be a programming assignment, some questions will ask you to \"write a few sentences\" in markdown cells. \n",
    "\n",
    "Submission Instructions:\n",
    "\n",
    "Create a labs directory in your personal class repository (e.g., located in your home directory)\n",
    "Clone the class repository\n",
    "Copy this Jupyter notebook file (.ipynb) into your repo/labs directory\n",
    "Make your edits, commit changes, and push to your repository\n",
    "All submissions must be pushed before the due date to avoid late penalties. \n",
    "\n",
    "Labs are graded out of a 100 pts. Each day late is -10. For a max penalty of -50 after 5 days. From there you may submit the lab anytime before the semester ends for a max score of 50.  \n",
    "\n",
    "Lab 10 is due on 5/15/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9ce705-af82-4e4b-8f80-4055fce3b598",
   "metadata": {},
   "source": [
    "# Missing Data\n",
    "\n",
    "Load up the Boston Housing Data and separate into matrix `X` for the features and vector `y` for the response. Randomize the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6e7b03-f331-4a15-af29-0d6ef21c5fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#Load the Boston housing data from the provided URL\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=r\"\\s+\", skiprows=22, header=None)\n",
    "\n",
    "\n",
    "#This creates a full set of predictors for each observation.\n",
    "X_data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "\n",
    "#The response y is stored in the third column of the second row of each pair.\n",
    "y = raw_df.values[1::2, 2]\n",
    "\n",
    "#Add an intercept column (like R's model.matrix does automatically)\n",
    "X = sm.add_constant(X_data)\n",
    "\n",
    "#Define column names using the provided names\n",
    "col_names = np.concatenate(([\"Intercept\"], \n",
    "                            [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \n",
    "                             \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\"]))\n",
    "\n",
    "# Convert the numpy arrays to pandas DataFrames with column names\n",
    "X_df = pd.DataFrame(X, columns=col_names)\n",
    "y_df = pd.DataFrame(y, columns=[\"medv\"])\n",
    "\n",
    "#  the dimensions to verify\n",
    "print(\"Number of predictors (including intercept):\", X_df.shape[1])\n",
    "print(\"Number of observations:\", X_df.shape[0])\n",
    "\n",
    "#Display the first few rows of the DataFrames\n",
    "print(X_df.head())\n",
    "print(y_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3ef680-c310-49e2-90ed-3f1aecb1e58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = \n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87f3f7c-fee7-4f57-98d3-e1e35eb870f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Randomize the rows by shuffling the indices\n",
    "shuffled_indices = \n",
    "combined_df = \n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09573da9-31af-43df-a75d-445939079e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This removes the 'medv' column from combined_df and returns it as a Series, which we then convert to a DataFrame.\n",
    "y_df = \n",
    "X_df = \n",
    "\n",
    "print()\n",
    "X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fc4800-9ff0-48aa-8eb0-23e87a508f4a",
   "metadata": {},
   "source": [
    "Similar to lab 1, write a function that takes a matrix and punches holes (i.e. sets entries equal to `NA`) randomly with an argument `prob_missing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4322a764-50bb-4f11-b5bf-72b46be87ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def punch_holes(matrix, prob_missing):\n",
    "    \"\"\"\n",
    "    Randomly sets entries in a matrix to NA based on the given probability.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    matrix : pandas.DataFrame or numpy.ndarray\n",
    "        The input matrix in which to punch holes.\n",
    "    prob_missing : float\n",
    "        The probability (between 0 and 1) that any given entry is set to NA.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    matrix_with_holes : same type as input\n",
    "        A copy of the original matrix with entries randomly set to NA.\n",
    "    \"\"\"\n",
    "    #Create a boolean mask with True values where we want to set NA\n",
    "    mask = np.random.choice([True, False], size=matrix.shape, p=[prob_missing, 1 - prob_missing])\n",
    "    \n",
    "    #If matrix is a DataFrame, make a copy and set the mask locations to np.nan\n",
    "    if isinstance(matrix, pd.DataFrame):\n",
    "        matrix_with_holes = matrix.copy()\n",
    "        matrix_with_holes[mask] = np.nan\n",
    "    #If matrix is a numpy array, copy it and set the mask locations to np.nan\n",
    "    elif isinstance(matrix, np.ndarray):\n",
    "        matrix_with_holes = matrix.copy()\n",
    "        #If the array has an integer dtype, convert to float so it can hold np.nan\n",
    "        if np.issubdtype(matrix_with_holes.dtype, np.integer):\n",
    "            matrix_with_holes = matrix_with_holes.astype(float)\n",
    "        matrix_with_holes[mask] = np.nan\n",
    "    else:\n",
    "        raise TypeError(\"Input must be a pandas DataFrame or a numpy array\")\n",
    "    \n",
    "    return matrix_with_holes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fe0a7d-4ba1-43bc-8402-1caa5e4fac43",
   "metadata": {},
   "source": [
    "Create a matrix `Xmiss` which is `X` but has missingness with probability of 10% using the function you just wrote. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aee8f07-775c-4e8f-a1a8-f2e3a5e7c6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Xmiss by introducing 10% missingness using the punch_holes function:\n",
    "Xmiss = \n",
    "\n",
    "#Display Xmiss to verify the missing entries\n",
    "print(\"Matrix X with 10% missing values:\")\n",
    "print(Xmiss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fbe572-06b1-4a16-99e5-9123b5611af1",
   "metadata": {},
   "source": [
    "Also, generate the M matrix and delete columns that have no missingness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77952d92-97d1-4324-9c4d-51ee85d57ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the indicator matrix M: Each entry in M is 1 if the corresponding entry in Xmiss is NA, and 0 otherwise.\n",
    "M = \n",
    "\n",
    "#Rename the columns of M by prefixing \"is_missing_\" to the original column names.\n",
    "M.columns = \n",
    "\n",
    "#Delete (i.e. drop) columns that have no missingness (columns whose sum is 0).\n",
    "M = \n",
    "\n",
    "#print M to verify\n",
    "print(\"Indicator matrix M (only columns with missing values):\")\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4db4449-00b0-4edf-b862-80352d0229cb",
   "metadata": {},
   "source": [
    "Split the first 400 observations were the training data and the remaining observations are the test set. For Xmiss, cbind on the M so the model has a chance to fit on \"is missing\" as we discussed in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c802513-02d6-4a8e-8d89-8bd92a65f2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Combine Xmiss and M (column-wise, similar to R's cbind)\n",
    "Xmiss_with_M = \n",
    "\n",
    "#Split the data: The first 400 observations will be the training set,The remaining observations will be the test set.\n",
    "\n",
    "X_train, X_test, y_train, y_test = \n",
    "\n",
    "#Do the same for the combined Xmiss_with_M:\n",
    "Xmiss_train, Xmiss_test, _, _ = \n",
    "\n",
    "#Verify the shapes\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"Xmiss_train shape:\", Xmiss_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"Xmiss_test shape:\", Xmiss_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bb919c-ae87-4601-b794-98956179e503",
   "metadata": {},
   "source": [
    "Fit a random forest model of `y_train ~ X_train`, report oos s_e (not oob) on `X_test`. This ignores missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438d79c6-547d-4e93-8846-8298dee40674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#Instantiate and fit the random forest model on X_train and y_train\n",
    "rf = \n",
    "rf.\n",
    "\n",
    "#Predict on the test set\n",
    "y_pred =\n",
    "\n",
    "#Compute the out-of-sample standard error (root mean squared error)\n",
    "oos_se = \n",
    "print(\"Out-of-sample standard error (RMSE):\", oos_se)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a431192c-7ce1-46ec-9c93-ff15d40b0a7b",
   "metadata": {},
   "source": [
    "Impute the missingness in `Xmiss` using the feature averages to create a matrix `Ximp_naive_train` and `Ximp_naive_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c9c4b3-4c98-4a9b-ac61-e4951e6eb00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#Create an imputer that replaces missing values with the column mean\n",
    "imputer = \n",
    "\n",
    "#Fit the imputer on the training data and transform it\n",
    "Ximp_naive_train = \n",
    "\n",
    "#Use the fitted imputer to transform the test data\n",
    "Ximp_naive_test = \n",
    "\n",
    "#Print a preview to verify\n",
    "print(\"Imputed Training Data:\")\n",
    "print(Ximp_naive_train.head())\n",
    "print(\"Imputed Test Data:\")\n",
    "print(Ximp_naive_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88fecca-080c-4bd5-b78e-595235b3345d",
   "metadata": {},
   "source": [
    "Fit a random forest model of `y_train ~ Ximp_naive_train`, report oos s_e (not oob) on `Ximp_naive_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d69127b-30ea-4fcd-b8b7-238e4e94ba02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "#Instantiate the random forest regressor\n",
    "rf_naive = \n",
    "\n",
    "#Fit the model on the imputed training data\n",
    "rf_naive.\n",
    "\n",
    "#Predict on the imputed test data\n",
    "y_pred_naive = \n",
    "\n",
    "#Compute the out-of-sample standard error (using RMSE as the metric)\n",
    "oos_se_naive = \n",
    "print(\"Out-of-sample standard error (RMSE):\", oos_se_naive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d850122-ae7e-4071-b4d6-628a41d2423e",
   "metadata": {},
   "source": [
    "How much predictive performance was lost due to missingness when naive imputation was used vs when there was no missingness?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbb33fc-2fc5-47c9-a763-b70025a57e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Complete data model OOS SE (RMSE):\", oos_se)\n",
    "print(\"Naive imputation model OOS SE (RMSE):\", oos_se_naive)\n",
    "\n",
    "#Calculate the absolute loss in performance (increase in RMSE)\n",
    "performance_loss = \n",
    "print(\"Absolute performance loss (RMSE difference):\", performance_loss)\n",
    "\n",
    "#Calculate the relative performance loss as a percentage\n",
    "relative_loss = \n",
    "print(\"Relative performance loss (%):\", relative_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8ba180-3960-4c70-a158-12409882a26e",
   "metadata": {},
   "source": [
    "Use `missForest` to impute the missing entries to create a matrix `Ximp_MF_train` and `Ximp_MF_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d84896-f237-4162-a237-370b49f66663",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer  \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "#Create an IterativeImputer using a RandomForestRegressor as the estimator. This is conceptually similar to missForest.\n",
    "imputer_MF = \n",
    "\n",
    "#Fit the imputer on the training data and transform it\n",
    "Ximp_MF_train = \n",
    "\n",
    "#Use the fitted imputer to transform the test data\n",
    "Ximp_MF_test = \n",
    "\n",
    "#Print a preview to verify\n",
    "print(\"Imputed Training Data with MissForest-like Imputation:\")\n",
    "print(Ximp_MF_train.head())\n",
    "print(\"Imputed Test Data with MissForest-like Imputation:\")\n",
    "print(Ximp_MF_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664e4e5d-5e94-4fa2-a16b-c586cb54824e",
   "metadata": {},
   "source": [
    "Code for to handle numerical and categorial columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cb7fce-49b1-4a6c-b8ea-c683ee087067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer  \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.base import clone\n",
    "\n",
    "def missforest_impute(X_train, X_test):\n",
    "    X_train = X_train.copy()\n",
    "    X_test = X_test.copy()\n",
    "\n",
    "    #Separate columns by dtype\n",
    "    num_cols = X_train.select_dtypes(include=[\"number\"]).columns\n",
    "    cat_cols = X_train.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns\n",
    "\n",
    "    #Encode categorical columns\n",
    "    cat_maps = {}\n",
    "    for col in cat_cols:\n",
    "        categories = X_train[col].astype(\"category\").cat.categories\n",
    "        cat_maps[col] = categories\n",
    "        X_train[col] = X_train[col].astype(\"category\").cat.codes.replace(-1, np.nan)\n",
    "        X_test[col] = pd.Categorical(X_test[col], categories=categories).codes\n",
    "        X_test[col] = X_test[col].replace(-1, np.nan)\n",
    "\n",
    "    # Impute numerical columns\n",
    "    if len(num_cols) > 0:\n",
    "        imputer_num = IterativeImputer(\n",
    "            estimator=RandomForestRegressor(n_estimators=10, random_state=0),\n",
    "            max_iter=10,\n",
    "            random_state=0\n",
    "        )\n",
    "        X_train[num_cols] = imputer_num.fit_transform(X_train[num_cols])\n",
    "        X_test[num_cols] = imputer_num.transform(X_test[num_cols])\n",
    "\n",
    "    #Impute categorical columns\n",
    "    if len(cat_cols) > 0:\n",
    "        imputer_cat = IterativeImputer(\n",
    "            estimator=RandomForestClassifier(n_estimators=10, random_state=0),\n",
    "            max_iter=10,\n",
    "            random_state=0\n",
    "        )\n",
    "        X_train[cat_cols] = imputer_cat.fit_transform(X_train[cat_cols])\n",
    "        X_test[cat_cols] = imputer_cat.transform(X_test[cat_cols])\n",
    "\n",
    "        #Round, convert to int, and map back to original categories\n",
    "        for col in cat_cols:\n",
    "            X_train[col] = X_train[col].round().astype(int)\n",
    "            X_test[col] = X_test[col].round().astype(int)\n",
    "            X_train[col] = pd.Categorical.from_codes(X_train[col], cat_maps[col])\n",
    "            X_test[col] = pd.Categorical.from_codes(X_test[col], cat_maps[col])\n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "#Example usage\n",
    "Ximp_train, Ximp_test = missforest_impute(Xmiss_train, Xmiss_test)\n",
    "\n",
    "#Preview\n",
    "print(\"Imputed Training Data:\")\n",
    "print(Ximp_train.head())\n",
    "print(\"Imputed Test Data:\")\n",
    "print(Ximp_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf121b1-8772-4bb2-a841-fa981543249f",
   "metadata": {},
   "source": [
    "Fit a random forest model of `y_train ~ Ximp_MF_train`, report oos s_e (not oob) on `Ximp_MF_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc40b575-70ae-4d5d-b05a-f6b941b7fe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#Instantiate the random forest regressor\n",
    "rf_MF = \n",
    "\n",
    "#Fit the model on the missForest-imputed training data\n",
    "rf_MF.\n",
    "\n",
    "#Predict on the missForest-imputed test data\n",
    "y_pred_MF = \n",
    "\n",
    "#Compute the out-of-sample standard error (using RMSE as the metric)\n",
    "oos_se_MF = np.sqrt(mean_squared_error(y_test, y_pred_MF))\n",
    "print(\"Out-of-sample standard error (RMSE) for missForest-imputed data:\", oos_se_MF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27436224-3873-413e-9ed4-8b3ddd419ab1",
   "metadata": {},
   "source": [
    "How much predictive performance was lost due to missingness when `missForest` imputation was used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2b75fe-1247-4e4c-8768-293c277b4b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Complete data model OOS SE (RMSE):\", oos_se)\n",
    "print(\"missForest-imputed model OOS SE (RMSE):\", oos_se_MF)\n",
    "\n",
    "# Calculate the absolute loss in performance (increase in RMSE)\n",
    "performance_loss_MF = oos_se_MF - oos_se\n",
    "print(\"Absolute performance loss (RMSE difference):\", performance_loss_MF)\n",
    "\n",
    "# Calculate the relative performance loss as a percentage\n",
    "relative_loss_MF = (performance_loss_MF / oos_se) * 100\n",
    "print(\"Relative performance loss (%):\", relative_loss_MF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793a810f-fa62-492b-9bc7-ea6d6861495e",
   "metadata": {},
   "source": [
    "Why did `missForest` imputation perform better than naive imputation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4709ab-82ee-415d-b9e4-ab56a2f985a0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "444f95e8-d7aa-44a5-8e46-f566ada62755",
   "metadata": {},
   "source": [
    "Reload the feature matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c3dbec-b5a6-435b-9737-d37d67161643",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_df)\n",
    "X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1ad83d-676e-4aba-807a-632472086124",
   "metadata": {},
   "source": [
    "## Bagged Trees and Random Forest\n",
    "\n",
    "Take a training sample of n = 2000 observations from the diamonds data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4370bdf1-9fb7-4b5a-955c-982bbf9d468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine.data import diamonds\n",
    "\n",
    "#Load the diamonds dataset from plotnine\n",
    "\n",
    "#Display the first few rows to verify the data\n",
    "\n",
    "\n",
    "#Take a random training sample of 2000 observations from the dataset\n",
    "sample_df = diamonds_df.sample()\n",
    "\n",
    "#Define the predictor matrix (X_df) and the response vector (y_df) Assuming 'price' is the response variable and all other columns are predictors.\n",
    "X_df = sample_df.drop()\n",
    "y_df = sample_df[]\n",
    "\n",
    "#Reset the index\n",
    "X_train = X_df.reset_index(drop=True)\n",
    "y_train = \n",
    "\n",
    "#Verify the shapes of the training sets\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f067a9f-e0d2-44c4-bb22-2d60f51dbd65",
   "metadata": {},
   "source": [
    "Using the diamonds data, find the oob s_e for a bagged-tree model using 1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000 trees. If you are using the `randomForest` package, you can create the bagged tree model via setting an argument within the RF constructor function. Plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9061bdaf-59b9-4db8-b28e-8dac931ee59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import ggplot, aes, geom_line, geom_point, labs, theme_minimal\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#X_train and y_train are already defined from your sampling of 2000 observations.\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "\n",
    "#Since the diamonds dataset has categorical variables, create dummy variables.\n",
    "X_train_dummies = \n",
    "\n",
    "#Define the list of tree counts to evaluate.\n",
    "tree_counts = \n",
    "\n",
    "#For each number of trees, fit a bagged-tree model using a random forest with all features at each split.\n",
    "results = []\n",
    "for :\n",
    "   \n",
    "    # Fit the model using the training data; flatten y_train to a 1D array.\n",
    "    \n",
    "    \n",
    "    # Get the out-of-bag predictions and compute the RMSE.\n",
    "    oob_pred = \n",
    "    rmse =\n",
    "    results.append({'n_trees': n_trees, 'oob_rmse': rmse})\n",
    "\n",
    "# Convert results to a DataFrame for plotting.\n",
    "results_df =\n",
    "print(results_df)\n",
    "\n",
    "# Plot OOB RMSE vs. Number of Trees using plotnine.\n",
    "plot = (ggplot(results_df, aes(x='n_trees', y='oob_rmse')) +\n",
    "        geom_line() +\n",
    "        geom_point() +\n",
    "        labs(title='OOB RMSE vs. Number of Trees (Bagged Trees)', x='Number of Trees', y='OOB RMSE') +\n",
    "        theme_minimal()\n",
    "       )\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1743d695-1e6e-48c6-ab75-962588d7ebca",
   "metadata": {},
   "source": [
    "Note: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates ... This warning means that for some of the models (especially those with very few trees), there weren't enough out‐of‐bag (OOB) predictions available to compute a reliable OOB score. In other words, when the number of trees is very low, some training samples might not be left out of the bootstrap samples often enough, which leads to unstable or missing OOB estimates. As you increase the number of trees, the OOB score becomes more reliable because each sample has more opportunities to be left out during training.\n",
    "\n",
    "\n",
    "These results suggest that the ensemble's performance improves dramatically as you add more trees, especially when moving from 1 or 2 trees to 10 trees. With just one tree, the OOB RMSE is very high (~4785), indicating that a single model is not reliable. As you increase the number of trees, the OOB RMSE drops sharply (to ~1293 with 10 trees, then ~1042 with 20 trees), meaning that the bagging process is stabilizing the predictions.\n",
    "\n",
    "Beyond about 30–40 trees, the RMSE levels off around 990–1000. Increasing the number of trees from 40 up to 1000 only yields marginal improvements (with the RMSE decreasing from around 999 to about 989). This flattening indicates that the ensemble has reached a point where adding more trees does not significantly improve predictive performance—the model's variance has been largely reduced, and further increases in tree count yield diminishing returns.\n",
    "\n",
    "In summary, using an ensemble of trees (bagged trees) substantially improves performance compared to a single tree, but there is a point of diminishing returns, as evidenced by the nearly constant OOB RMSE after approximately 40 trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b9f8e8-9af4-4659-8a51-7aa939d272ac",
   "metadata": {},
   "source": [
    "Find the bootstrap s_e for a RF model using 1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000 trees. If you are using the `randomForest` package, you can calculate oob residuals via `e_oob = y_train - rf_mod$predicted`. Plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8571e25-52b8-4bd2-897e-96756dc8836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import ggplot, aes, geom_line, geom_point, labs, theme_minimal\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "X_train_dummies = \n",
    "\n",
    "# List of tree counts to evaluate\n",
    "tree_counts = \n",
    "\n",
    "results = []\n",
    "for n_trees in tree_counts:\n",
    "    #Create a RandomForestRegressor that acts as a bagged tree model. Setting max_features equal to the number of predictors makes it a bagging model.\n",
    "    rf = \n",
    "    \n",
    "    #Fit the model\n",
    "    rf.\n",
    "    \n",
    "    #Compute out-of-bag predictions and residuals: e_oob = y_train - oob_prediction\n",
    "    oob_pred = \n",
    "    e_oob = \n",
    "    \n",
    "    #Calculate the bootstrap standard error as the RMSE of the OOB residuals.\n",
    "    bootstrap_se = np.sqrt(np.mean(e_oob ** 2))\n",
    "    \n",
    "    results.append({'n_trees': n_trees, 'bootstrap_se': bootstrap_se})\n",
    "\n",
    "#Convert results to a DataFrame for plotting.\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "#Plot bootstrap standard error vs. number of trees using plotnine.\n",
    "plot = (\n",
    "    ggplot(results_df, aes(x='n_trees', y='bootstrap_se')) +\n",
    "    geom_line() +\n",
    "    geom_point() +\n",
    "    labs(title='Bootstrap Standard Error vs. Number of Trees', x='Number of Trees', y='Bootstrap Standard Error (RMSE)') +\n",
    "    theme_minimal()\n",
    "    )\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdb9d9b-a027-40e1-b69a-a820c9f08482",
   "metadata": {},
   "source": [
    "What is the percentage gain / loss in performance of the RF model vs bagged trees model for each number of trees? Gains are negative (as in lower oos s_e)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11de186a-ea7f-46fd-a0a0-b335462ffe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import ggplot, aes, geom_line, geom_point, labs, theme_minimal\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Convert categorical predictors to dummy variables:\n",
    "X_train_dummies = pd.get_dummies(X_train, drop_first=True)\n",
    "\n",
    "# List of tree counts to evaluate.\n",
    "tree_counts = [1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000]\n",
    "\n",
    "results = []\n",
    "\n",
    "for n_trees in tree_counts:\n",
    "    # Bagged trees model: use all predictors at each split.\n",
    "    bagged = RandomForestRegressor(\n",
    "        n_estimators=n_trees,\n",
    "        oob_score=True,\n",
    "        bootstrap=True,\n",
    "        random_state=42,\n",
    "        max_features=X_train_dummies.shape[1]  # all predictors (bagging)\n",
    "    )\n",
    "    bagged.fit(X_train_dummies, y_train.values.ravel())\n",
    "    bagged_oob_pred = bagged.oob_prediction_\n",
    "    bagged_rmse = np.sqrt(mean_squared_error(y_train, bagged_oob_pred))\n",
    "    \n",
    "    # Random Forest model: use a random subset (\"sqrt\") of predictors at each split.\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=n_trees,\n",
    "        oob_score=True,\n",
    "        bootstrap=True,\n",
    "        random_state=42,\n",
    "        max_features=\"sqrt\"  # default for classification; for regression this induces randomness\n",
    "    )\n",
    "    rf.fit(X_train_dummies, y_train.values.ravel())\n",
    "    rf_oob_pred = rf.oob_prediction_\n",
    "    rf_rmse = np.sqrt(mean_squared_error(y_train, rf_oob_pred))\n",
    "    \n",
    "    # Calculate percentage gain/loss in performance.\n",
    "    # Percentage gain is computed as:\n",
    "    #    (rf_rmse - bagged_rmse) / bagged_rmse * 100\n",
    "    # So if rf_rmse is lower (better) than bagged_rmse, the gain will be negative.\n",
    "    pct_gain = (rf_rmse - bagged_rmse) / bagged_rmse * 100\n",
    "    \n",
    "    results.append({\n",
    "        'n_trees': n_trees,\n",
    "        'bagged_oob_rmse': bagged_rmse,\n",
    "        'rf_oob_rmse': rf_rmse,\n",
    "        'pct_gain': pct_gain\n",
    "    })\n",
    "\n",
    "# Convert the results to a DataFrame.\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Plot percentage gain/loss vs. number of trees using plotnine.\n",
    "plot = (\n",
    "    ggplot(results_df, aes(x='n_trees', y='pct_gain')) +\n",
    "    geom_line() +\n",
    "    geom_point() +\n",
    "    labs(\n",
    "        title='Percentage Gain/Loss in OOB RMSE: RF vs. Bagged Trees',\n",
    "        x='Number of Trees',\n",
    "        y='Percentage Gain (negative is better for RF)'\n",
    "    ) +\n",
    "    theme_minimal()\n",
    ")\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e166babc-5f29-4a8d-9738-21e841e9f762",
   "metadata": {},
   "source": [
    "Why was this the result?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99a745f-7d6a-452b-8394-7647e880a9b9",
   "metadata": {},
   "source": [
    "Your results indicate that for nearly every tree count, the RF model (which selects a random subset of predictors at each split) has a slightly higher OOB RMSE than the bagged trees model (which uses all predictors at every split). In your table, a positive pct_gain means that the RF model's error is higher relative to the bagged trees model (i.e. RF is performing worse), except for the case with 2 trees where RF is very slightly better.\n",
    "\n",
    "One interpretation is that in this particular dataset, all predictors appear to be quite informative. When you allow each tree in the ensemble to use all predictors (bagging), the trees can fully exploit the available information, which leads to lower prediction error. In contrast, by forcing each tree to only consider a subset of predictors (as in the RF model), you might be leaving out some of the valuable signals that improve prediction, hence the higher OOB RMSE.\n",
    "\n",
    "Additionally, the differences are relatively small (generally a few percent), suggesting that while RF’s random feature selection can reduce correlation among trees and help prevent overfitting in some contexts, here it introduces a slight loss in performance compared to using all predictors. The performance gain (or rather, loss) from RF versus bagging depends on the underlying data structure, and in this case, the diamonds data seem to favor the bagged approach.\n",
    "\n",
    "\n",
    "Note: In some datasets, the RF model—with its random subset of predictors at each split—can outperform bagged trees. The key difference is that RF's random feature selection can reduce the correlation among trees and help prevent overfitting, especially when predictors are noisy or highly correlated. In such cases, the diversity among the trees helps the ensemble generalize better, leading to improved performance. However, if most or all predictors are very informative and not redundant, bagging (which uses all predictors) might capture the full signal more effectively, as you observed in your results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372d35f7-26af-4055-98b1-7aef1b8e5843",
   "metadata": {},
   "source": [
    "Plot oob s_e by number of trees for both RF and bagged trees by creating a long data frame from the two results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b7bdd4-26a3-4a55-9b9f-77b0296e02ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import ggplot, aes, geom_line, geom_point, labs, theme_minimal\n",
    "\n",
    "#Convert the wide format results_df to long format\n",
    "results_long = pd.melt(results_df, \n",
    "                       id_vars=[\"n_trees\"], \n",
    "                       value_vars=[\"bagged_oob_rmse\", \"rf_oob_rmse\"],\n",
    "                       var_name=\"model\", \n",
    "                       value_name=\"oob_rmse\")\n",
    "\n",
    "#Modify the model names to be more readable\n",
    "results_long['model'] = results_long['model'].map({\n",
    "    'bagged_oob_rmse': 'Bagged Trees',\n",
    "    'rf_oob_rmse': 'Random Forest'\n",
    "})\n",
    "\n",
    "#Plot OOB SE (RMSE) vs. number of trees, colored by model\n",
    "plot = (ggplot(results_long, aes(x='n_trees', y='oob_rmse', color='model')) +\n",
    "        geom_line() +\n",
    "        geom_point() +\n",
    "        labs(title='OOB Standard Error (RMSE) vs. Number of Trees', x='Number of Trees', y='OOB SE (RMSE)', color='Model') +\n",
    "        theme_minimal()\n",
    "        )\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e8b935-e8f2-4fa7-bf6b-89748d4a2d20",
   "metadata": {},
   "source": [
    "Build RF models for 500 trees using different `mtry` values: 1, 2, ... the maximum. That maximum will be the number of features assuming that we do not binarize categorical features if you are using `randomForest`. Calculate oob s_e for all mtry values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bd2248-8268-493c-8916-e1b0666ed910",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine.data import diamonds\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from plotnine import ggplot, aes, geom_line, geom_point, labs, theme_minimal\n",
    "\n",
    "#Load the diamonds dataset\n",
    "diamonds_df = diamonds.copy()\n",
    "\n",
    "#Take a random sample of 2000 observations\n",
    "sample_df = diamonds_df.sample(n=2000, random_state=42)\n",
    "X_raw = sample_df.drop(columns=['price'])\n",
    "y_raw = sample_df['price']\n",
    "\n",
    "#Convert categorical features to ordinal numeric codes (i.e., do not one-hot encode)\n",
    "for col in X_raw.columns:\n",
    "    \n",
    "\n",
    "# Determine the maximum mtry: the total number of predictors\n",
    "max_mtry = X_raw.shape[1]\n",
    "\n",
    "results = []\n",
    "# Iterate mtry from 1 to max_mtry\n",
    "for mtry in range(1, max_mtry + 1):\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=,\n",
    "        oob_score=,\n",
    "        bootstrap=,\n",
    "        random_state=42,\n",
    "        max_features=  # mtry value\n",
    "    )\n",
    "    rf.fit(X_raw, y_raw)\n",
    "    # OOB predictions are stored in oob_prediction_\n",
    "    oob_pred = rf.oob_prediction_\n",
    "    oob_rmse = np.sqrt(mean_squared_error(y_raw, oob_pred))\n",
    "    results.append({'mtry': mtry, 'oob_rmse': oob_rmse})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c911d9-2e27-4135-bdf3-60bb53d14d81",
   "metadata": {},
   "source": [
    "Plot oob s_e by mtry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7f4239-904a-4e31-8ec6-78ef1a8d9641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the OOB standard error vs. mtry using plotnine.\n",
    "plot = (ggplot(results_df, aes(x='mtry', y='oob_rmse')) +\n",
    "        geom_line() +\n",
    "        geom_point() +\n",
    "        labs(title='OOB Standard Error vs. mtry (500 Trees)',\n",
    "        x='mtry (Number of Features Considered per Split)',\n",
    "        y='OOB Standard Error (RMSE)') +\n",
    "        theme_minimal()\n",
    "        )\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21b639e-4abe-4de7-b90e-4a8663d68410",
   "metadata": {},
   "source": [
    "Take a sample of n = 2000 observations from the adult data and name it `adult_sample`. Then impute missing values using missForest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a519bd3b-fdf3-4d67-8de1-d77760b2df88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "np.random.seed(1)\n",
    "adult = pd.read_csv(\"adult_data.csv\")  # Replace with the actual path or data loading method\n",
    "\n",
    "#Take a random sample of 2000 observations from the adult dataset.\n",
    "adult_sample = adult.sample()\n",
    "\n",
    "missing_counts = adult_sample.isna().sum()\n",
    "print(missing_counts)\n",
    "adult_sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47333532-f676-43b4-a86a-24e3d71df1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of categorical columns to encode.\n",
    "categorical_columns = ['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country', 'income']\n",
    "\n",
    "#List of the continuous columns\n",
    "continuous_columns = ['age', 'fnlwgt', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']\n",
    "\n",
    "#Convert these columns to float\n",
    "adult_sample[continuous_columns] = adult_sample[continuous_columns].\n",
    "\n",
    "#Verify the conversion\n",
    "print(adult_sample[continuous_columns].)\n",
    "\n",
    "#Create a copy of the data for encoding.\n",
    "adult_sample_encoded = adult_sample.copy()\n",
    "\n",
    "#Dictionary to store mapping for each categorical column.\n",
    "mappings = {}\n",
    "\n",
    "# Loop through each categorical column.\n",
    "for col in categorical_columns:\n",
    "    # Convert the column to a pandas Categorical type.\n",
    "    \n",
    "    \n",
    "    # Save the mapping: integer codes to original category names.\n",
    "    mappings[col] = dict(enumerate(adult_sample_encoded[col].cat.categories))\n",
    "    \n",
    "    # Replace the column with its numeric codes.\n",
    "    adult_sample_encoded[col] = adult_sample_encoded[col].\n",
    "\n",
    "#Now, adult_sample_encoded has numeric codes for the categorical columns, and the mappings dictionary retains the original category names.\n",
    "print(\"Mappings for categorical columns:\")\n",
    "for col, mapping in mappings.items():\n",
    "    print(f\"{col}: {mapping}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62114ecd-9301-4cd0-8b4b-f25c158cbc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#Impute missing values using a missForest-like approach on the encoded data.\n",
    "imputer = \n",
    "\n",
    "#Use adult_sample_encoded (which now has numeric values) for imputation.\n",
    "adult_sample_imputed_array = imputer.fit_transform()\n",
    "adult_sample_imputed = pd.DataFrame(, columns=)\n",
    "\n",
    "#Verify the imputation by viewing the first few rows.\n",
    "print(adult_sample_imputed.head())\n",
    "\n",
    "#Check to ensure there are no missing values/nans\n",
    "missing_counts = adult_sample_imputed.isna().sum()\n",
    "print(missing_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc657d9-343a-4e42-9f49-f9750ca3d2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate predictors and target.\n",
    "X_train = adult_sample_imputed.drop(columns=[])\n",
    "y_train = adult_sample_imputed[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f1579b-7d1a-4fcb-984d-f2fcf2ab4887",
   "metadata": {},
   "source": [
    "Using the adult_train data, find the bootstrap misclassification error for a bagged-tree model using 1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000 trees. Plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbe15b6-3d1d-4971-b24f-304de1453bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from plotnine import ggplot, aes, geom_line, geom_point, labs, theme_minimal\n",
    "\n",
    "\n",
    "#For bagging, set max_features to the total number of predictors.\n",
    "max_features = X_train.shape[1]\n",
    "\n",
    "#Define the list of tree counts to evaluate.\n",
    "tree_counts = [1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000]\n",
    "\n",
    "results = []\n",
    "for n_trees in tree_counts:\n",
    "    \n",
    "    #Create a bagged tree classifier: using all features at each split.\n",
    "    clf = \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    #The oob_score_ attribute gives OOB accuracy.\n",
    "    oob_accuracy = clf.oob_score_\n",
    "    misclassification_error = 1 - oob_accuracy  # bootstrap misclassification error\n",
    "    \n",
    "    results.append({'n_trees': n_trees, 'bagged_misclassification_error': misclassification_error})\n",
    "\n",
    "bagged_results_df = pd.DataFrame(results)\n",
    "print(bagged_results_df)\n",
    "\n",
    "# Plot the OOB misclassification error versus number of trees using plotnine.\n",
    "plot = (ggplot(results_df, aes(x='n_trees', y='misclassification_error')) +\n",
    "        geom_line() +\n",
    "        geom_point() +\n",
    "        labs(title='Bootstrap Misclassification Error vs. Number of Trees (Bagged Trees)', x='Number of Trees', y='Misclassification Error') +\n",
    "        theme_minimal()\n",
    "        )\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba25acd-5923-4043-a0ec-2a04aa437f71",
   "metadata": {},
   "source": [
    "Using the adult_train data, find the bootstrap misclassification error for an RF model using 1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000 trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7b075c-67b3-49a3-a60b-c977a209e1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Separate predictors and target.\n",
    "X_train = adult_sample_imputed.drop(columns=['income'])\n",
    "y_train = adult_sample_imputed['income']\n",
    "\n",
    "#Define the list of tree counts to evaluate.\n",
    "tree_counts = [1, 2, 5, 10, 20, 30, 40, 50, 100, 200, 300, 400, 500, 1000]\n",
    "\n",
    "results = []\n",
    "\n",
    "for n_trees in tree_counts:\n",
    "    #Build a RandomForestClassifier using the default max_features (for classification this is typically \"sqrt\")\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=,\n",
    "        oob_score=,\n",
    "        bootstrap=,\n",
    "        random_state=42\n",
    "        # We do not override max_features, so it uses the default.\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    #OOB accuracy is stored in oob_score_\n",
    "    oob_accuracy = rf.oob_score_\n",
    "    misclassification_error =   # bootstrap misclassification error\n",
    "    \n",
    "    results.append({'n_trees': n_trees,'rf_misclassification_error': misclassification_error})\n",
    "\n",
    "#Convert the results to a DataFrame and print\n",
    "rf_results_df = pd.DataFrame(results)\n",
    "print(rf_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527e089b-af35-4b2a-ab0b-e9093879ccc9",
   "metadata": {},
   "source": [
    "What is the percentage gain / loss in performance of the RF model vs bagged trees model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdf2b5e-47ad-431e-9961-5d6a91813012",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Merge the two dataframes on 'n_trees'\n",
    "merged_results = bagged_results_df.merge()\n",
    "\n",
    "merged_results\n",
    "\n",
    "#Calculate the percentage gain/loss in performance: Formula: (rf_error - bagged_error) / bagged_error * 100\n",
    "merged_results['pct_gain'] =\n",
    "\n",
    "#Display the resulting dataframe\n",
    "print(merged_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d52eba-ee39-4cf7-8767-882fda00c5b3",
   "metadata": {},
   "source": [
    "Note: We divide by the bagged tree error because that error serves as our baseline for comparison. In this calculation, we're expressing the change in performance of the RF model relative to the bagged trees model. By using the bagged error as the denominator, the percentage gain (or loss) tells you how much the RF model's performance deviates from the baseline performance of the bagged trees. \n",
    "\n",
    "If we divided by the RF error instead, it would change the interpretation. The current approach makes it clear that any improvement (or worsening) is measured as a percentage of the bagged trees' error. This is particularly useful if you consider the bagged trees model as the standard or reference model in your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e1d3d7-6bca-418f-b1b3-1c01b651a695",
   "metadata": {},
   "source": [
    "Build RF models on adult_train for 500 trees using different `mtry` values: 1, 2, ... the maximum (see above as maximum is defined by the specific RF algorithm implementation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e49a2b-10e5-474a-90d6-89d0305b1357",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from plotnine import ggplot, aes, geom_line, geom_point, labs, theme_minimal\n",
    "\n",
    "#The maximum mtry value is the total number of features.\n",
    "max_mtry = \n",
    "\n",
    "results = []\n",
    "for mtry in range(1, max_mtry + 1):\n",
    "    \n",
    "    rf.fit(X_train, y_train)\n",
    "    # Compute the OOB misclassification error (1 - OOB accuracy)\n",
    "    oob_error = \n",
    "    results.append({'mtry': mtry, 'oob_error': oob_error})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fabff3-422d-43d9-8eb0-11e5389d33fc",
   "metadata": {},
   "source": [
    "Plot bootstrap misclassification error by `mtry`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b323de-ecde-458a-85e6-99a2ab2b9303",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the OOB misclassification error versus mtry using plotnine.\n",
    "plot = (ggplot(results_df, aes(x='mtry', y='oob_error')) +\n",
    "        geom_line() +\n",
    "        geom_point() +\n",
    "        labs(title='OOB Misclassification Error vs. mtry (500 Trees)', \n",
    "             x='mtry (Number of Features Considered per Split)', y='OOB Misclassification Error') +\n",
    "        theme_minimal()\n",
    "        )\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fddd2aa-5893-4c79-88f9-1812e942668c",
   "metadata": {},
   "source": [
    "Is `mtry` an important hyperparameter to optimize when using the RF algorithm? Explain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e65269-ce9c-47e6-b4b4-57cf5be5c08d",
   "metadata": {},
   "source": [
    "Yes, `mtry` is an important hyperparameter to optimize when using the Random Forest (RF) algorithm because it controls the number of features randomly selected at each split in the decision trees.\n",
    "\n",
    "A well-chosen `mtry` value helps balance two key trade-offs:\n",
    "\n",
    "1. **Tree diversity vs. tree strength:**  \n",
    "   A smaller `mtry` increases the diversity between trees in the forest, which can reduce overfitting by lowering the correlation among trees. However, if it's too small, each individual tree becomes weak because it doesn’t have enough relevant features to split on.  \n",
    "   A larger `mtry` allows each tree to be stronger, but it also increases the similarity between trees, which can reduce the benefit of averaging in the ensemble.\n",
    "\n",
    "2. **Model performance:**  \n",
    "   The right `mtry` value can lead to better generalization and lower out-of-bag (OOB) or test error. The default in scikit-learn is `sqrt(p)` for classification and `p/3` for regression, but these defaults may not be optimal for all datasets.\n",
    "\n",
    "Therefore, tuning `mtry`—just like other hyperparameters such as the number of trees or maximum depth—can help you achieve better predictive performance from a Random Forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe225a3-de09-43a1-85c5-e1f0f3fba52a",
   "metadata": {},
   "source": [
    "Identify the best model among all values of `mtry`. Fit this RF model. Then report the following oob error metrics: misclassification error, precision, recall, F1, FDR, FOR and compute a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966de3f1-13af-492a-bf57-952de8420b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "#Identify the best mtry value based on minimum OOB misclassification error.\n",
    "best_mtry = results_df.loc[results_df['oob_error'].idxmin(), 'mtry']\n",
    "print(\"Best mtry value:\", best_mtry)\n",
    "\n",
    "#Fit the best RF model using 500 trees with the best mtry.\n",
    "best_rf = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    oob_score=True,\n",
    "    bootstrap=True,\n",
    "    random_state=42,\n",
    "    max_features=best_mtry\n",
    ")\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "#Get the OOB decision function (probabilities). For binary classification, oob_decision_function_ returns an array with 2 columns.\n",
    "#We use the probability for the positive class (column index 1) and threshold at 0.5.\n",
    "oob_proba = best_rf.oob_decision_function_\n",
    "oob_pred = (oob_proba[:, 1] >= 0.5).astype(int)\n",
    "\n",
    "#Calculate metrics.\n",
    "accuracy = accuracy_score(y_train, oob_pred)\n",
    "misclassification_error = 1 - accuracy\n",
    "precision = precision_score(y_train, oob_pred, pos_label=1)\n",
    "recall = recall_score(y_train, oob_pred, pos_label=1)\n",
    "f1 = f1_score(y_train, oob_pred, pos_label=1)\n",
    "\n",
    "#Compute the confusion matrix.\n",
    "cm = confusion_matrix(y_train, oob_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "#Calculate FDR (False Discovery Rate) and FOR (False Omission Rate)\n",
    "FDR = fp / (fp + tp) if (fp + tp) > 0 else 0\n",
    "FOR = fn / (fn + tn) if (fn + tn) > 0 else 0\n",
    "\n",
    "#Create a DataFrame summarizing the metrics.\n",
    "metrics_df = pd.DataFrame({'Metric': ['Misclassification Error', 'Precision', 'Recall', 'F1 Score', 'FDR', 'FOR'],\n",
    "    'Value': [misclassification_error, precision, recall, f1, FDR, FOR]\n",
    "})\n",
    "\n",
    "print(\"OOB Error Metrics for the best RF model (500 trees, mtry = {}):\".format(best_mtry))\n",
    "print(metrics_df)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76ea083-a225-4fec-9278-05b52766ebf1",
   "metadata": {},
   "source": [
    "Is this a good model? (yes/no and explain)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525cabbc-a3bb-4d35-8fa7-cb89dfbe26c6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4089b0e5-55eb-4994-9e68-2ac76b9e6b09",
   "metadata": {},
   "source": [
    "There are probability asymmetric costs to the two types of errors. Assign two costs below and calculate oob total cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acd3ec2-204a-4f98-b344-0f720132909b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign cost values (adjust these values as appropriate for your application)\n",
    "fp_cost =    # Cost assigned to a false positive\n",
    "fn_cost =    # Cost assigned to a false negative\n",
    "\n",
    "# Using the previously computed confusion matrix components: tn, fp, fn, tp\n",
    "# Compute the total cost as:\n",
    "total_cost = \n",
    "\n",
    "print(\"OOB Total Cost:\", total_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0abc898-3659-44ca-a60c-c3750d24d466",
   "metadata": {},
   "source": [
    "# Asymmetric Cost Modeling, ROC and DET curves\n",
    "\n",
    "Fit a logistic regression model to the adult_train missingness-imputed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa7db71-4677-49d3-959c-59387b51a3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "#Separate predictors (X) and target (y).\n",
    "X_train = adult_sample_imputed.drop(columns=['income'])\n",
    "y_train = adult_sample_imputed['income']\n",
    "\n",
    "#Scale the data\n",
    "scaler = \n",
    "X_train_scaled = scaler.\n",
    "\n",
    "#Fit a logistic regression model with increased max_iter\n",
    "logistic_model = \n",
    "logistic_model.fit()\n",
    "\n",
    "print(\"Model coefficients:\", logistic_model.)\n",
    "print(\"Model intercept:\", logistic_model.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8324e72b-b7b0-4b16-ba80-879f0658417f",
   "metadata": {},
   "source": [
    "Use the function from class to calculate all the error metrics (misclassification error, precision, recall, F1, FDR, FOR) for the values of the probability threshold being 0.001, 0.002, ..., 0.999 in a tibble (dplyr data frame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7c4537-c990-4012-98a6-e471ce912fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted probabilities for the positive class.\n",
    "p_hat = logistic_model.predict_proba(X_train_scaled)[:, 1]\n",
    "\n",
    "#Define thresholds from 0.001 to 0.999\n",
    "thresholds = np.arange(0.001, 1, 0.001)\n",
    "\n",
    "#Total number of samples (for misclassification error calculation)\n",
    "n = len(y_train)\n",
    "\n",
    "results = []\n",
    "for thresh in thresholds:\n",
    "    #Binary predictions based on threshold\n",
    "    y_pred = (p_hat >= thresh).astype(int)\n",
    "    \n",
    "    #Compute confusion matrix: it returns an array [[TN, FP], [FN, TP]]\n",
    "    cm = confusion_matrix(y_train, y_pred)\n",
    "    \n",
    "    #Handle the case if one of the classes is missing in predictions\n",
    "    if cm.shape == (2, 2):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "    else:\n",
    "        #If only one class is present in y_pred, assign accordingly.\n",
    "        if np.unique(y_pred).size == 1:\n",
    "            if y_pred[0] == 0:\n",
    "                tn = cm[0, 0]\n",
    "                fp = 0\n",
    "                fn = np.sum(y_train)  #All actual positives missed.\n",
    "                tp = 0\n",
    "            else:\n",
    "                tn = 0\n",
    "                fp = np.sum(y_train == 0)\n",
    "                fn = 0\n",
    "                tp = cm[0, 0]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    misclassification_error = (fp + fn) / n\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    FDR = fp / (tp + fp) if (tp + fp) > 0 else 0  # False Discovery Rate\n",
    "    FOR = fn / (fn + tn) if (fn + tn) > 0 else 0  # False Omission Rate\n",
    "    \n",
    "    results.append({\n",
    "        'p_hat_threshold': thresh,\n",
    "        'misclassification_error': misclassification_error,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'F1': f1,\n",
    "        'FDR': FDR,\n",
    "        'FOR': FOR,\n",
    "        'FP': fp,\n",
    "        'FN': fn\n",
    "    })\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the first few rows of the results\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7945b0e9-909a-4e2a-af5b-ede2e3e17775",
   "metadata": {},
   "source": [
    "Calculate the column `total_cost` and append it to this data frame via `mutate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29730768-54bd-4c8f-81e1-199506dfb4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the cost per false positive and false negative\n",
    "fp_cost =    # Cost for a false positive\n",
    "fn_cost =    # Cost for a false negative\n",
    "\n",
    "#Append a new column 'total_cost' using the assigned cost values:\n",
    "results_df = results_df.assign()\n",
    "\n",
    "#Display the first few rows of the final DataFrame\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03873d3c-0fd9-4131-876e-33306d170372",
   "metadata": {},
   "source": [
    "Which is the lowest total cost? What is the \"winning\" probability threshold value providing that minimum total cost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d52956-8eb0-4499-95fb-a4ae7c0d4353",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the row with the minimum total cost\n",
    "min_cost_row = results_df.loc[]\n",
    "\n",
    "#Extract the lowest total cost and the corresponding probability threshold\n",
    "lowest_total_cost = \n",
    "winning_threshold =\n",
    "\n",
    "print(\"Lowest Total Cost:\", lowest_total_cost)\n",
    "print(\"Winning Probability Threshold:\", winning_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10756636-3e32-436e-bbe4-77c093c65bd9",
   "metadata": {},
   "source": [
    "Plot an ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15c6f97-e1d0-43f3-97be-cc81df68a675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from plotnine import ggplot, aes, geom_line, labs, theme_minimal\n",
    "\n",
    "\n",
    "#Compute ROC curve metrics\n",
    "fpr, tpr, thresholds = \n",
    "auc_value = \n",
    "print(\"AUC:\", auc_value)\n",
    "\n",
    "#Create a DataFrame to hold the ROC curve data\n",
    "roc_data = pd.DataFrame()\n",
    "\n",
    "#Plot the ROC curve using plotnine\n",
    "roc_plot = (ggplot(roc_data, aes(x='False Positive Rate', y='True Positive Rate')) +\n",
    "            geom_line(color='blue') +\n",
    "            labs(title=f'ROC Curve (AUC = {auc_value:.2f})', x='False Positive Rate (FPR)', y='True Positive Rate (TPR)') +\n",
    "            theme_minimal())\n",
    "roc_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79819ab-bc2e-4dea-9fdd-b9d1866d5f44",
   "metadata": {},
   "source": [
    "Interpret the ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae12af9-7356-41ee-be98-a4211fded224",
   "metadata": {},
   "source": [
    "The ROC curve shows how well the model distinguishes between the two classes by plotting the True Positive Rate (TPR) against the False Positive Rate (FPR) for various probability thresholds. An AUC of 0.87 indicates that the model has a strong ability to separate the positive class from the negative class. Specifically:\n",
    "\n",
    "- **Curve Shape:** The curve rises quickly toward the top‐left corner, meaning that at many threshold values, the model achieves a high TPR while keeping the FPR relatively low.  \n",
    "- **AUC = 0.87:** Since a value of 0.5 would correspond to random guessing, 0.87 is considered good. It suggests that, on average, the model can correctly rank a positive instance higher than a negative instance 87% of the time.  \n",
    "\n",
    "Overall, the model demonstrates solid discriminative power and should perform well in scenarios where correctly identifying positive cases is important, provided that the chosen threshold balances the desired trade‐off between false positives and false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c56402b-3a50-4ecf-ba0f-8aaed485b73f",
   "metadata": {},
   "source": [
    "Calculate AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0cbca3-367c-49c1-9435-ff775ca98fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_value = \n",
    "print(\"AUC:\", auc_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee19e70-5f97-49f7-a868-9850cda97243",
   "metadata": {},
   "source": [
    "Interpret the AUC results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7126fa04-9a31-46b2-a4f9-2836b40abe0e",
   "metadata": {},
   "source": [
    "An AUC of approximately 0.865 indicates that the model has strong discriminatory power. In practical terms, it means that if you randomly select one positive case and one negative case, there is about an 86.5% chance that the model will assign a higher predicted probability to the positive case than to the negative one. This suggests the model is effective at distinguishing between the two classes, performing well above random chance (which would be 0.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d332739b-e894-4c5c-b725-05f630b25c78",
   "metadata": {},
   "source": [
    "Plot a DET curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0c83a2-f72f-4e1d-ad14-fa78a0f34899",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "#Get predicted probabilities for the positive class\n",
    "p_hat = \n",
    "\n",
    "#Compute the ROC curve to obtain FPR and TPR.\n",
    "fpr, tpr, thresholds = \n",
    "fnr = # False Negative Rate\n",
    "\n",
    "# To avoid infinities in the probit transformation, clip FPR and FNR\n",
    "epsilon = 1e-5\n",
    "fpr_clipped =\n",
    "fnr_clipped = \n",
    "\n",
    "# Apply the probit (inverse normal CDF) transformation.\n",
    "det_x = norm.ppf\n",
    "det_y =\n",
    "\n",
    "# Create a DataFrame for plotting the DET curve.\n",
    "det_df = \n",
    "\n",
    "# Plot the DET curve using plotnine.\n",
    "det_plot = (ggplot(det_df, aes(x='det_x', y='det_y')) +\n",
    "            geom_line(color='blue') +\n",
    "            labs(title='DET Curve', x='False Positive Rate (probit scale)', y='False Negative Rate (probit scale)') +\n",
    "            theme_minimal()\n",
    "            )\n",
    "det_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ad2e93-501d-442b-b58c-baeb66527078",
   "metadata": {},
   "source": [
    "Interpret the DET curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d220b1-d98a-4250-89ab-6a5a0699d379",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
